#!/usr/bin/env python3
"""
å¹¶è¡Œç­›é€‰ä¸»æ§åˆ¶å™¨
è´Ÿè´£ç³»ç»Ÿæ£€æµ‹ã€æ™ºèƒ½åˆ†é…ã€è¿›ç¨‹ç®¡ç†å’Œæ–­ç‚¹ç»­ä¼ 
"""

import os
import sys
import json
import time
import psutil
import multiprocessing as mp
from datetime import datetime
from pathlib import Path
import traceback
import shutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, os.path.join(project_root, 'src'))
sys.path.insert(0, os.path.join(project_root, 'tools', 'xml_splitter'))
sys.path.insert(0, os.path.join(project_root, 'i18n'))

# ç¡®ä¿èƒ½å¤Ÿæ­£ç¡®å¯¼å…¥ i18n_manager
i18n_path = os.path.join(project_root, 'i18n')
if i18n_path not in sys.path:
    sys.path.append(i18n_path)

# å¯¼å…¥å›½é™…åŒ–ç®¡ç†å™¨
try:
    from i18n.i18n_manager import get_message
    I18N_AVAILABLE = True
except ImportError:
    I18N_AVAILABLE = False
    print("Warning: i18n module not available, using fallback messages")

# ç®€åŒ–çš„XMLSplitterç±»ï¼ˆé¿å…å¯¼å…¥é—®é¢˜ï¼‰
class XMLSplitter:
    def count_records(self, xml_path):
        import xml.etree.ElementTree as ET
        tree = ET.parse(xml_path)
        root = tree.getroot()
        records = root.findall('.//record')
        return len(records)

from src.utils import load_config

# å›½é™…åŒ–æ¶ˆæ¯å¤„ç†å‡½æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨i18nç³»ç»Ÿï¼‰
def get_message_fallback(key, **kwargs):
    """
    è·å–æœ¬åœ°åŒ–æ¶ˆæ¯
    ä¼˜å…ˆä½¿ç”¨i18nç³»ç»Ÿï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨å›é€€æ¶ˆæ¯
    """
    if I18N_AVAILABLE:
        try:
            # ä½¿ç”¨hasattrå’Œglobalså®‰å…¨æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
            import sys
            current_module = sys.modules[__name__]
            if hasattr(current_module, 'get_message') and callable(getattr(current_module, 'get_message', None)):
                return current_module.get_message(key, **kwargs)
        except Exception:
            pass
    
    # å›é€€æ¶ˆæ¯æ˜ å°„ï¼ˆè‹±æ–‡ï¼‰
    fallback_messages = {
        "config_loaded": "âœ“ Configuration file loaded successfully",
        "temp_prepared": "âœ“ Temporary directory prepared: {temp_dir}",
        "system_detection": "ğŸ–¥ï¸  System Resource Detection",
        "cpu_cores": "CPU Cores: {cores}",
        "total_memory": "Total Memory: {memory:.1f}GB",
        "available_memory": "Available Memory: {memory:.1f}GB",
        "available_disk": "Available Disk Space: {space:.1f}GB",
        "recommended_screeners": "Recommended Screeners: {count}",
        "max_safe_screeners": "Max Safe Screeners: {count}",
        "config_warning": "âš ï¸  Configuration Warning:",
        "suggestion": "ğŸ’¡ Suggestion:",
        "adjust_to_recommended": "Adjust to recommended configuration ({count} screeners)? [y/N]: ",
        "adjusted_to": "âœ“ Adjusted to {count} screeners",
        "continue_anyway": "Continue with current configuration anyway? [y/N]: ",
        "execution_cancelled": "Execution cancelled",
        "exceed_cpu_cores": "Requested screeners ({requested}) exceed safe CPU cores ({safe})",
        "recommend_cpu_screeners": "Recommend setting to {count} screeners (based on CPU cores)",
        "exceed_memory": "Estimated memory usage ({estimated:.1f}GB) may exceed available memory ({available:.1f}GB)",
        "recommend_memory_screeners": "Based on memory limit, recommend setting to {count} screeners",
        "insufficient_disk": "Disk space less than 2GB, may affect temporary file storage",
        "debug_screener_count": "ğŸ” Debug: Using {count} screeners (from configuration file)",
        "screener_modified_warning": "âš ï¸  Warning: Screener count was unexpectedly modified!",
        "screener_reset_success": "âœ“ Reset to configuration value: {count}",
        "all_batches_completed_status": "âœ… All batches processing completed",
        "wait_process_error": "âš ï¸  Error waiting for processes to complete: {error}",
        "reload_state_file": "ğŸ“‹ Reloading state file: {file}",
        "state_file_not_exist": "âš ï¸  State file does not exist, using passed state",
        "cannot_get_state": "Unable to get state information",
        "batch_status_stats": "ğŸ“Š Batch status: {completed}/{total} completed",
        "save_state_failed": "âš ï¸  Failed to save state: {error}",
        "temp_cleanup_success": "âœ“ Temporary files cleaned",
        "temp_cleanup_error": "âš ï¸  Error cleaning temporary files: {error}",
        "parallel_screening_system": "ğŸ¯ SmartEBM Parallel Screening System",
        "parallel_screening_completed": "âœ… Parallel screening completed",
        "parallel_screening_failed": "âŒ Parallel screening failed",
        "user_interrupted": "âš ï¸  User interrupted operation",
        "detected_incomplete_task": "ğŸ”„ Detected incomplete screening task",
        "task_id": "Task ID: {id}",
        "total_records_label": "Total Records: {count}",
        "screener_count_label": "Screener Count: {count}",
        "current_progress": "Current Progress: {completed}/{total} ({percent:.1f}%)",
        "pending_batches_label": "Pending Batches:",
        "options_label": "Options:",
        "auto_continue_batches": "1. Auto continue incomplete batches",
        "restart_all_tasks": "2. Restart all tasks",
        "cancel_operation": "3. Cancel",
        "please_select_option": "Please select [1/2/3]: ",
        "please_enter_valid_option": "Please enter a valid option (1/2/3)",
        "auto_continue_selected": "âœ“ Auto continue incomplete tasks",
        "restart_selected": "âœ“ Restart tasks",
        "cancelled_selected": "Cancelled",
        "startup_failed": "âŒ Startup failed: {error}",
        "new_task_startup_failed": "âŒ New task startup failed: {error}",
        "parallel_execution_starting": "ğŸ”„ Starting parallel screening execution...",
        "parallel_execution_failed": "âŒ Parallel screening execution failed: {error}",
        "resume_screening_failed": "âŒ Resume screening failed: {error}",
        "execution_cancelled_user": "Execution cancelled",
        "starting_new_parallel": "ğŸš€ Starting parallel screening system"
    }
    
    message = fallback_messages.get(key, key)
    if message and kwargs:
        try:
            return message.format(**kwargs)
        except (KeyError, ValueError):
            return message
    return message or key


class SystemCapacityDetector:
    """ç³»ç»Ÿèµ„æºæ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_system_capacity():
        """æ£€æµ‹ç³»ç»Ÿèµ„æºå¹¶æ¨èé…ç½®"""
        try:
            # CPUæ ¸å¿ƒæ•°
            cpu_cores = os.cpu_count() or 4  # é»˜è®¤å€„4æ ¸
            
            # å†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            available_memory_gb = memory.available / (1024**3)
            total_memory_gb = memory.total / (1024**3)
            
            # ç£ç›˜ç©ºé—´
            disk_usage = psutil.disk_usage('/')
            available_disk_gb = disk_usage.free / (1024**3)
            
            # æ¨èé…ç½®ï¼ˆä¿å®ˆä¼°ç®—ï¼‰
            # æ¯ä¸ªç­›é€‰å™¨å¤§çº¦éœ€è¦512MBå†…å­˜
            memory_based_limit = int(available_memory_gb // 0.5)
            cpu_based_limit = max(1, cpu_cores - 1)  # ä¿ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
            
            recommended_screeners = min(memory_based_limit, cpu_based_limit, 8)  # æœ€å¤š8ä¸ª
            
            return {
                'cpu_cores': cpu_cores,
                'total_memory_gb': round(total_memory_gb, 2),
                'available_memory_gb': round(available_memory_gb, 2),
                'available_disk_gb': round(available_disk_gb, 2),
                'recommended_screeners': max(1, recommended_screeners),
                'max_safe_screeners': cpu_based_limit,
                'memory_per_screener_mb': 512
            }
        except Exception as e:
            print(f"System detection error: {str(e)}")
            # è¿”å›ä¿å®ˆé»˜è®¤å€¼
            return {
                'cpu_cores': 2,
                'total_memory_gb': 8.0,
                'available_memory_gb': 4.0,
                'available_disk_gb': 10.0,
                'recommended_screeners': 2,
                'max_safe_screeners': 2,
                'memory_per_screener_mb': 512
            }
    
    @staticmethod
    def validate_parallel_config(parallel_config, system_capacity):
        """éªŒè¯å¹¶è¡Œé…ç½®æ˜¯å¦åˆç†"""
        requested_screeners = parallel_config.get('parallel_screeners', 1)
        
        warnings = []
        recommendations = []
        
        # CPUæ£€æŸ¥
        if requested_screeners > system_capacity['max_safe_screeners']:
            warnings.append(
                get_message_fallback("exceed_cpu_cores", 
                          requested=requested_screeners, 
                          safe=system_capacity['max_safe_screeners'])
            )
            recommendations.append(
                get_message_fallback("recommend_cpu_screeners", 
                          count=system_capacity['max_safe_screeners'])
            )
        
        # å†…å­˜æ£€æŸ¥
        estimated_memory = requested_screeners * system_capacity['memory_per_screener_mb'] / 1024
        if estimated_memory > system_capacity['available_memory_gb'] * 0.8:
            warnings.append(
                get_message_fallback("exceed_memory", 
                          estimated=estimated_memory, 
                          available=system_capacity['available_memory_gb'])
            )
            memory_safe_screeners = int(system_capacity['available_memory_gb'] * 0.8 * 1024 / 512)
            recommendations.append(
                get_message_fallback("recommend_memory_screeners", 
                          count=memory_safe_screeners)
            )
        
        # ç£ç›˜ç©ºé—´æ£€æŸ¥
        if system_capacity['available_disk_gb'] < 2:
            warnings.append(get_message_fallback("insufficient_disk"))
        
        return {
            'is_safe': len(warnings) == 0,
            'warnings': warnings,
            'recommendations': recommendations,
            'adjusted_screeners': min(
                requested_screeners,
                system_capacity['recommended_screeners']
            )
        }


class RecordDistributor:
    """æ™ºèƒ½è®°å½•åˆ†é…å™¨"""
    
    @staticmethod
    def calculate_distribution(total_records, parallel_screeners):
        """è®¡ç®—è®°å½•åˆ†é…æ–¹æ¡ˆ"""
        if parallel_screeners <= 0:
            raise ValueError("Number of screeners must be greater than 0")
        
        if total_records <= 0:
            raise ValueError("Number of records must be greater than 0")
        
        # åŸºç¡€åˆ†é…é‡
        base_count = total_records // parallel_screeners
        remainder = total_records % parallel_screeners
        
        distributions = []
        current_start = 1
        
        for i in range(parallel_screeners):
            # å‰remainderä¸ªç­›é€‰å™¨å¤šåˆ†é…1æ¡è®°å½•
            current_count = base_count + (1 if i < remainder else 0)
            current_end = current_start + current_count - 1
            
            distributions.append({
                'batch_id': i + 1,
                'start_record': current_start,
                'end_record': current_end,
                'record_count': current_count,
                'status': 'pending'
            })
            
            current_start = current_end + 1
        
        return distributions
    
    @staticmethod
    def print_distribution_table(distributions, total_records):
        """æ‰“å°åˆ†é…è¡¨æ ¼"""
        print("\n" + "="*70)
        print("ğŸ“‹ Record Distribution Plan")
        print("="*70)
        print(f"{'Batch':<6} {'Start Record':<12} {'End Record':<10} {'Record Count':<12} {'Percentage':<10}")
        print("-"*70)
        
        for dist in distributions:
            percentage = (dist['record_count'] / total_records) * 100
            print(f"{dist['batch_id']:<6} {dist['start_record']:<12} "
                  f"{dist['end_record']:<10} {dist['record_count']:<12} "
                  f"{percentage:.1f}%")
        
        print("-"*70)
        print(f"{'Total':<6} {'':<12} {'':<10} {total_records:<12} {'100.0%':<10}")
        print("="*70)


class ParallelScreeningManager:
    """å¹¶è¡Œç­›é€‰ç®¡ç†å™¨"""
    
    def __init__(self, config_path="config.json"):
        self.config_path = config_path
        self.config: dict = {}      # ç»Ÿä¸€é…ç½®
        self.state_file = ""
        self.temp_dir = ""
        self.processes = {}
        self.batch_status = {}
        
        # åŠ è½½é…ç½®
        self.load_configurations()
        
        # åˆå§‹åŒ–ç³»ç»Ÿæ£€æµ‹
        self.system_capacity = SystemCapacityDetector.detect_system_capacity()
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.setup_temp_directory()
    
    def load_configurations(self):
        """åŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶"""
        try:
            # åŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶
            self.config = load_config(self.config_path)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹¶è¡Œé…ç½®
            if 'parallel_settings' not in self.config:
                raise Exception("Configuration file missing parallel_settings section")
            
            # è®¾ç½®çŠ¶æ€æ–‡ä»¶è·¯å¾„
            self.state_file = self.config['parallel_settings'].get('state_file', 'parallel_screening_state.json')
            
            print(get_message_fallback("config_loaded"))
            
        except Exception as e:
            raise Exception(f"Configuration file loading failed: {str(e)}")
    
    def setup_temp_directory(self):
        """è®¾ç½®ä¸´æ—¶ç›®å½•"""
        if not self.config:
            raise Exception("Configuration file not loaded")
            
        self.temp_dir = self.config['parallel_settings']['temp_dir']
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        os.makedirs(self.temp_dir, exist_ok=True)
        print(get_message_fallback("temp_prepared", temp_dir=self.temp_dir))
    
    def print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        capacity = self.system_capacity
        
        print("\n" + "="*60)
        print(get_message_fallback("system_detection"))
        print("="*60)
        print(get_message_fallback("cpu_cores", cores=capacity['cpu_cores']))
        print(get_message_fallback("total_memory", memory=capacity['total_memory_gb']))
        print(get_message_fallback("available_memory", memory=capacity['available_memory_gb']))
        print(get_message_fallback("available_disk", space=capacity['available_disk_gb']))
        print(get_message_fallback("recommended_screeners", count=capacity['recommended_screeners']))
        print(get_message_fallback("max_safe_screeners", count=capacity['max_safe_screeners']))
        print("="*60)
    
    def validate_configuration(self):
        """éªŒè¯é…ç½®"""
        if not self.config:
            raise Exception("Configuration file not loaded")
            
        validation = SystemCapacityDetector.validate_parallel_config(
            self.config['parallel_settings'],
            self.system_capacity
        )
        
        if not validation['is_safe']:
            print(f"\n{get_message_fallback('config_warning')}")
            for warning in validation['warnings']:
                print(f"   - {warning}")
            
            print(f"\n{get_message_fallback('suggestion')}")
            for rec in validation['recommendations']:
                print(f"   - {rec}")
            
            # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
            choice = input(f"\n{get_message_fallback('adjust_to_recommended', count=validation['adjusted_screeners'])}")
            if choice.lower() == 'y':
                self.config['parallel_settings']['parallel_screeners'] = validation['adjusted_screeners']
                print(get_message_fallback('adjusted_to', count=validation['adjusted_screeners']))
            else:
                choice = input(get_message_fallback('continue_anyway'))
                if choice.lower() != 'y':
                    print(get_message_fallback('execution_cancelled'))
                    return False
        
        return True
    
    def count_xml_records(self, xml_path):
        """ç»Ÿè®¡XMLè®°å½•æ•°é‡"""
        try:
            splitter = XMLSplitter()
            return splitter.count_records(xml_path)
        except Exception as e:
            raise Exception(f"XML record counting failed: {str(e)}")
    
    def check_existing_state(self):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªå®Œæˆçš„ä»»åŠ¡"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„æ‰¹æ¬¡
                pending_batches = [b for b in state['batches'] 
                                 if b['status'] in ['pending', 'failed', 'running']]
                
                if pending_batches:
                    return state, pending_batches
                    
            except Exception as e:
                print(f"State file read failed: {str(e)}")
        
        return None, []
    
    def interactive_recovery(self, state, pending_batches):
        """Interactive recovery selection"""
        print("\n" + "="*60)
        print(get_message_fallback("detected_incomplete_task"))
        print("="*60)
        print(get_message_fallback("task_id", id=state['session_id']))
        print(get_message_fallback("total_records_label", count=state['total_records']))
        print(get_message_fallback("screener_count_label", count=state['parallel_screeners']))
        
        completed = sum(1 for b in state['batches'] if b['status'] == 'completed')
        total = len(state['batches'])
        progress = (completed / total) * 100
        
        print(get_message_fallback("current_progress", completed=completed, total=total, percent=progress))
        
        print(f"\n{get_message_fallback('pending_batches_label')}")
        for batch in pending_batches:
            status_icon = {"pending": "â³", "failed": "âŒ", "running": "ğŸ”„"}
            icon = status_icon.get(batch['status'], "â“")
            print(f"  {icon} Batch {batch['batch_id']}: Records {batch['start_record']}-{batch['end_record']} ({batch['status']})")
        
        print(f"\n{get_message_fallback('options_label')}")
        print(get_message_fallback("auto_continue_batches"))
        print(get_message_fallback("restart_all_tasks"))
        print(get_message_fallback("cancel_operation"))
        
        while True:
            choice = input(f"\n{get_message_fallback('please_select_option')}").strip()
            if choice in ['1', '2', '3']:
                return choice
            print(get_message_fallback('please_enter_valid_option'))
    
    def _ensure_config_loaded(self):
        """ç¡®ä¿é…ç½®å·²åŠ è½½"""
        if not self.config:
            raise Exception("Configuration file not loaded, please call load_configurations() first")
    
    def create_session_state(self, total_records, distributions):
        """åˆ›å»ºæ–°çš„ä¼šè¯çŠ¶æ€"""
        self._ensure_config_loaded()
        
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        state = {
            'session_id': session_id,
            'created_at': datetime.now().isoformat(),
            'total_records': total_records,
            'parallel_screeners': self.config['parallel_settings']['parallel_screeners'],
            'input_xml_path': self.config['paths']['input_xml_path'],
            'batches': distributions,
            'temp_dir': self.temp_dir
        }
        
        # ä¿å­˜çŠ¶æ€æ–‡ä»¶
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ Session state saved: {session_id}")
        return state
    
    def start_parallel_screening(self):
        """å¯åŠ¨å¹¶è¡Œç­›é€‰"""
        try:
            print(f"\n{get_message_fallback('starting_new_parallel')}")
            
            # 1. æ‰“å°ç³»ç»Ÿä¿¡æ¯
            self.print_system_info()
            
            # 2. éªŒè¯é…ç½®
            if not self.validate_configuration():
                return False
            
            # 3. æ£€æŸ¥ç°æœ‰çŠ¶æ€
            existing_state, pending_batches = self.check_existing_state()
            
            if existing_state and pending_batches:
                choice = self.interactive_recovery(existing_state, pending_batches)
                
                if choice == '1':
                    # ç»§ç»­æœªå®Œæˆä»»åŠ¡
                    print(get_message_fallback("auto_continue_selected"))
                    return self.resume_screening(existing_state, pending_batches)
                elif choice == '2':
                    # Restart task
                    print(get_message_fallback("restart_selected"))
                    self.cleanup_temp_files()
                elif choice == '3':
                    print(get_message_fallback("cancelled_selected"))
                    return False
            
            # 4. å¼€å§‹æ–°ä»»åŠ¡
            return self.start_new_screening()
            
        except Exception as e:
            print(f"âŒ Startup failed: {str(e)}")
            traceback.print_exc()
            return False
    
    def start_new_screening(self):
        """å¼€å§‹æ–°çš„ç­›é€‰ä»»åŠ¡"""
        try:
            self._ensure_config_loaded()
            
            # ç»Ÿè®¡è®°å½•æ•°é‡
            input_xml_path = self.config['paths']['input_xml_path']
            print(f"\nğŸ“– Analyzing input file: {os.path.basename(input_xml_path)}")
            
            total_records = self.count_xml_records(input_xml_path)
            print(f"âœ“ Detected {total_records} records")
            
            # è®¡ç®—åˆ†é…æ–¹æ¡ˆ
            parallel_screeners = self.config['parallel_settings']['parallel_screeners']
            
            # å¼ºåˆ¶æ£€æŸ¥ï¼šç¡®ä¿ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼Œä¸å…è®¸åŸºäºè®°å½•æ•°çš„è‡ªåŠ¨è°ƒæ•´
            print(get_message_fallback("debug_screener_count", count=parallel_screeners))
            if parallel_screeners != self.config['parallel_settings']['parallel_screeners']:
                print(get_message_fallback("screener_modified_warning"))
                parallel_screeners = self.config['parallel_settings']['parallel_screeners']
                print(get_message_fallback("screener_reset_success", count=parallel_screeners))
            
            distributions = RecordDistributor.calculate_distribution(total_records, parallel_screeners)
            
            # æ˜¾ç¤ºåˆ†é…è¡¨æ ¼
            RecordDistributor.print_distribution_table(distributions, total_records)
            
            # ç¡®è®¤æ‰§è¡Œ
            choice = input(f"\nStart parallel screening? [y/N]: ")
            if choice.lower() != 'y':
                print(get_message_fallback("execution_cancelled_user"))
                return False
            
            # åˆ›å»ºä¼šè¯çŠ¶æ€
            state = self.create_session_state(total_records, distributions)
            
            # æ‰§è¡Œåˆ†å‰²å’Œç­›é€‰
            return self.execute_parallel_screening(state)
            
        except Exception as e:
            print(f"âŒ New task startup failed: {str(e)}")
            traceback.print_exc()
            return False
    
    def execute_parallel_screening(self, state):
        """æ‰§è¡Œå¹¶è¡Œç­›é€‰"""
        try:
            print(f"\n{get_message_fallback('parallel_execution_starting')}")
            
            # 1. åˆ†å‰²XMLæ–‡ä»¶
            split_files = self.split_xml_file(state)
            if not split_files:
                return False
            
            # 2. ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºé…ç½®æ–‡ä»¶
            batch_configs = self.create_batch_configs(state, split_files)
            
            # 3. Start parallel screening processes
            return self.start_parallel_processes(state, batch_configs)
            
        except Exception as e:
            print(f"âŒ Parallel screening execution failed: {str(e)}")
            traceback.print_exc()
            return False
    
    def resume_screening(self, state, pending_batches):
        """æ¢å¤ç­›é€‰"""
        try:
            print("\nğŸ”„ Resuming screening processes...")
            
            # ä¸ºå¾…å¤„ç†æ‰¹æ¬¡åˆ›å»ºé…ç½®
            batch_configs = []
            for batch in pending_batches:
                batch_id = batch['batch_id']
                config_path = os.path.join(self.temp_dir, f"config_batch_{batch_id}.json")
                
                if os.path.exists(config_path):
                    batch_configs.append({
                        'batch_info': batch,
                        'config_path': config_path,
                        'split_file': os.path.join(self.temp_dir, f"batch_{batch_id}.xml")
                    })
                else:
                    print(f"âš ï¸  Batch {batch_id} configuration file missing, recreating...")
                    # é‡æ–°åˆ›å»ºé…ç½®æ–‡ä»¶
                    batch_config = self.create_single_batch_config(batch)
                    batch_configs.append(batch_config)
            
            # å¯åŠ¨æ¢å¤è¿›ç¨‹
            return self.start_parallel_processes(state, batch_configs)
            
        except Exception as e:
            print(f"âŒ Resume screening failed: {str(e)}")
            traceback.print_exc()
            return False
    
    def split_xml_file(self, state):
        """åˆ†å‰²XMLæ–‡ä»¶"""
        try:
            self._ensure_config_loaded()
            
            print("\nğŸ“„ Starting XML file splitting...")
            
            input_xml_path = self.config['paths']['input_xml_path']
            splitter = XMLSplitter()
            
            # ç”Ÿæˆåˆ†å‰²æ–‡ä»¶ååˆ—è¡¨
            split_files = []
            for batch in state['batches']:
                batch_id = batch['batch_id']
                split_file = os.path.join(self.temp_dir, f"batch_{batch_id}.xml")
                split_files.append(split_file)
            
            # ä½¿ç”¨è‡ªå®šä¹‰åˆ†å‰²é€»è¾‘
            self.custom_split_xml(input_xml_path, state['batches'], split_files)
            
            print(f"âœ“ XML splitting completed, generated {len(split_files)} files")
            return split_files
            
        except Exception as e:
            print(f"âŒ XML splitting failed: {str(e)}")
            return []
    
    def custom_split_xml(self, input_xml_path, batches, output_files):
        """è‡ªå®šä¹‰XMLåˆ†å‰²é€»è¾‘"""
        try:
            import xml.etree.ElementTree as ET
            
            # è§£æåŸå§‹XML
            tree = ET.parse(input_xml_path)
            root = tree.getroot()
            records = root.findall('.//record')
            
            print(f"Total records: {len(records)}")
            
            # æŒ‰æ‰¹æ¬¡åˆ†å‰²
            for i, batch in enumerate(batches):
                start_idx = batch['start_record'] - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
                end_idx = batch['end_record']  # end_recordæ˜¯åŒ…å«çš„
                
                # åˆ›å»ºæ–°çš„XMLç»“æ„
                new_root = ET.Element(root.tag)
                
                # å¤åˆ¶æ ¹å…ƒç´ å±æ€§
                for key, value in root.attrib.items():
                    new_root.set(key, value)
                
                # å¤åˆ¶érecordsçš„å­å…ƒç´ 
                for child in root:
                    if child.tag != 'records':
                        new_root.append(child)
                
                # åˆ›å»ºrecordså®¹å™¨
                records_elem = ET.SubElement(new_root, 'records')
                
                # æ·»åŠ è¯¥æ‰¹æ¬¡çš„è®°å½•
                batch_records = records[start_idx:end_idx]
                for record in batch_records:
                    records_elem.append(record)
                
                # ä¿å­˜æ–‡ä»¶
                new_tree = ET.ElementTree(new_root)
                ET.indent(new_tree, space="  ", level=0)
                new_tree.write(output_files[i], encoding='utf-8', xml_declaration=True)
                
                print(f"  Batch {batch['batch_id']}: {len(batch_records)} records -> {os.path.basename(output_files[i])}")
            
        except Exception as e:
            raise Exception(f"Custom splitting failed: {str(e)}")
    
    def create_batch_configs(self, state, split_files):
        """ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºé…ç½®æ–‡ä»¶"""
        try:
            print("\nâš™ï¸  Creating batch configuration files...")
            
            batch_configs = []
            
            for i, batch in enumerate(state['batches']):
                batch_id = batch['batch_id']
                split_file = split_files[i]
                
                # åˆ›å»ºæ‰¹æ¬¡ç‰¹å®šçš„é…ç½®
                batch_config = self.create_single_batch_config(batch, split_file)
                batch_configs.append(batch_config)
                
                print(f"  âœ“ Batch {batch_id} configuration created")
            
            return batch_configs
            
        except Exception as e:
            print(f"âŒ Batch configuration creation failed: {str(e)}")
            return []
    
    def create_single_batch_config(self, batch, split_file=None):
        """åˆ›å»ºå•ä¸ªæ‰¹æ¬¡çš„é…ç½®"""
        self._ensure_config_loaded()
        
        batch_id = batch['batch_id']
        
        if split_file is None:
            split_file = os.path.join(self.temp_dir, f"batch_{batch_id}.xml")
        
        # å¤åˆ¶ç»Ÿä¸€é…ç½®ï¼ˆå»é™¤å¹¶è¡Œéƒ¨åˆ†ï¼‰
        if not self.config:
            raise Exception("Unified configuration not loaded")
            
        import copy
        batch_config = copy.deepcopy(self.config)
        
        # è®¾ç½®ä¸ºå•çº¿ç¨‹æ¨¡å¼
        batch_config['mode']['screening_mode'] = 'single'
        
        # åˆ é™¤å¹¶è¡Œç›¸å…³é…ç½®
        if 'parallel_settings' in batch_config:
            del batch_config['parallel_settings']
        if 'resource_management' in batch_config:
            del batch_config['resource_management']
        if 'output_settings' in batch_config:
            del batch_config['output_settings']
        
        # ä¿®æ”¹è·¯å¾„é…ç½®
        batch_config['paths']['input_xml_path'] = os.path.abspath(split_file)
        batch_config['paths']['output_xml_path'] = os.path.abspath(
            os.path.join(self.temp_dir, f"batch_{batch_id}_results.xml")
        )
        
        # è®¾ç½®è·³è¿‡è®°å½•æ•°ä¸º0ï¼ˆå› ä¸ºå·²ç»åˆ†å‰²ï¼‰
        batch_config['processing']['skip_records_count'] = 0
        
        # ä¿å­˜æ‰¹æ¬¡é…ç½®æ–‡ä»¶
        config_path = os.path.join(self.temp_dir, f"config_batch_{batch_id}.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(batch_config, f, indent=2, ensure_ascii=False)
        
        return {
            'batch_info': batch,
            'config_path': config_path,
            'split_file': split_file
        }
    
    def start_parallel_processes(self, state, batch_configs):
        """å¯åŠ¨å¹¶è¡Œç­›é€‰è¿›ç¨‹"""
        try:
            self._ensure_config_loaded()
            
            import threading
            from datetime import datetime
            
            print(f"\nğŸš€ Starting {len(batch_configs)} parallel screening processes...")
            
            # æ›´æ–°çŠ¶æ€æ–‡ä»¶
            for batch_config in batch_configs:
                batch_info = batch_config['batch_info']
                batch_info['status'] = 'running'
                batch_info['started_at'] = datetime.now().isoformat()
                batch_info['config_path'] = batch_config['config_path']
                batch_info['split_file'] = batch_config['split_file']
            
            self.save_state(state)
            
            # å¯åŠ¨çº¿ç¨‹æ± 
            threads = []
            delay = self.config['resource_management'].get('delay_between_screeners', 2)
            
            for i, batch_config in enumerate(batch_configs):
                batch_id = batch_config['batch_info']['batch_id']
                config_path = batch_config['config_path']
                
                # åˆ›å»ºçº¿ç¨‹
                thread = threading.Thread(
                    target=self.run_single_batch,
                    args=(batch_id, config_path, state)
                )
                
                thread.start()
                threads.append(thread)
                
                print(f"  âœ“ Batch {batch_id} thread started")
                
                # å»¶è¿Ÿå¯åŠ¨ä¸‹ä¸€ä¸ªçº¿ç¨‹
                if i < len(batch_configs) - 1:
                    time.sleep(delay)
            
            # å¯åŠ¨ç›‘æ§
            print("\nğŸ“Š Starting progress monitoring...")
            from progress_monitor import ProgressMonitor
            self.progress_monitor = ProgressMonitor(self.state_file, 5)
            self.progress_monitor.start_monitoring()
            
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            print("\nâ³ Waiting for all batches to complete...")
            self.wait_for_completion_threads(threads, state)
            
            # åœæ­¢ç›‘æ§
            self.progress_monitor.stop_monitoring()
            
            # åˆå¹¶ç»“æœ
            print("\nğŸ”„ Merging screening results...")  
            return self.merge_final_results(state)
            
        except Exception as e:
            print(f"âŒ Parallel processes startup failed: {str(e)}")
            traceback.print_exc()
            return False
    
    def _execute_single_batch_screening(self, config_path):
        """ç›´æ¥æ‰§è¡Œå•ä¸ªæ‰¹æ¬¡çš„ç­›é€‰é€»è¾‘"""
        try:
            from src.extractor import SystematicReviewExtractor
            from src.utils import load_config, validate_config, create_output_directory, check_file_exists
            
            # åŠ è½½æ‰¹æ¬¡é…ç½®
            config = load_config(config_path)
            validate_config(config)
            
            # æå–é…ç½®å€¼
            paths = config['paths']
            processing = config.get('processing', {})
            
            # å¤„ç†ç ”ç©¶è®¾è®¡é…ç½®
            if 'study_designs' in config:
                excluded_designs = config['study_designs'].get('excluded_study_designs', [])
                included_designs = config['study_designs'].get('included_study_designs', [])
            else:
                excluded_designs = config.get('excluded_study_designs', [])
                included_designs = config.get('included_study_designs', [])
            
            llm_configs = config['llm_configs']
            inclusion_criteria = config['inclusion_criteria']
            exclusion_criteria = config.get('exclusion_criteria', {})
            
            # éªŒè¯è¾“å…¥æ–‡ä»¶
            input_xml_path = paths['input_xml_path']
            check_file_exists(input_xml_path, "Input XML file")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_xml_path = paths['output_xml_path']
            create_output_directory(output_xml_path)
            
            # åˆå§‹åŒ–æå–å™¨
            extractor = SystematicReviewExtractor(
                screening_llm_configs=llm_configs['screening_llms'],
                prompt_llm_config=llm_configs.get('prompt_llm'),
                positive_prompt_path=paths.get('positive_prompt_file_path'),
                negative_prompt_path=paths.get('negative_prompt_file_path')
            )
            
            # è§£æXMLæ–‡ä»¶
            parsed_records, tree, root = extractor.parse_xml(input_xml_path)
            
            # å¤„ç†è®°å½•è·³è¿‡
            skip_records_count = processing.get('skip_records_count', 0)
            if skip_records_count > 0:
                records_to_process = parsed_records[skip_records_count:]
            else:
                records_to_process = parsed_records
            
            # è®¾ç½®ç ”ç©¶è®¾è®¡è¿‡æ»¤
            extractor.set_excluded_study_designs(excluded_designs)
            extractor.set_included_study_designs(included_designs)
            
            # å¤„ç†è®°å½•
            extractor.process_records(
                records_to_process, 
                inclusion_criteria, 
                output_xml_path, 
                tree,
                exclusion_criteria=exclusion_criteria,
                prompt_file_path=paths.get('prompt_file_path')
            )
            
            return True
            
        except Exception as e:
            print(f"æ‰¹æ¬¡ç­›é€‰æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_single_batch(self, batch_id, config_path, state):
        """è¿è¡Œå•ä¸ªæ‰¹æ¬¡çš„ç­›é€‰"""
        try:
            from datetime import datetime
            
            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
            self.update_batch_status(state, batch_id, 'running', {
                'started_at': datetime.now().isoformat()
            })
            
            # ç›´æ¥è°ƒç”¨ç­›é€‰é€»è¾‘
            success = self._execute_single_batch_screening(config_path)
            
            if success:
                # æˆåŠŸå®Œæˆ
                # æ‰¾åˆ°å®é™…ç”Ÿæˆçš„Excelæ–‡ä»¶
                excel_pattern = os.path.join(self.temp_dir, f"batch_{batch_id}_results*.xlsx")
                import glob
                excel_files = glob.glob(excel_pattern)
                excel_file = excel_files[0] if excel_files else os.path.join(self.temp_dir, f"batch_{batch_id}_results.xlsx")
                
                self.update_batch_status(state, batch_id, 'completed', {
                    'completed_at': datetime.now().isoformat(),
                    'output_files': {
                        'xml': os.path.join(self.temp_dir, f"batch_{batch_id}_results.xml"),
                        'excel': excel_file
                    }
                })
                print(f"âœ… Batch {batch_id} completed")
            else:
                # å¤±è´¥
                self.update_batch_status(state, batch_id, 'failed', {
                    'failed_at': datetime.now().isoformat(),
                    'error': 'Screening process execution failed'
                })
                print(f"âŒ Batch {batch_id} failed")
                
        except Exception as e:
            # é”™è¯¯
            from datetime import datetime
            self.update_batch_status(state, batch_id, 'error', {
                'failed_at': datetime.now().isoformat(),
                'error': str(e)
            })
            print(f"ğŸ’¥ Batch {batch_id} error: {str(e)}")
            traceback.print_exc()
    
    def update_batch_status(self, state, batch_id, status, extra_data=None):
        """æ›´æ–°æ‰¹æ¬¡çŠ¶æ€"""
        try:
            # åŠ è½½æœ€æ–°çŠ¶æ€
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    current_state = json.load(f)
            else:
                current_state = state
            
            # æ›´æ–°æŒ‡å®šæ‰¹æ¬¡
            for batch in current_state['batches']:
                if batch['batch_id'] == batch_id:
                    batch['status'] = status
                    if extra_data:
                        batch.update(extra_data)
                    break
            
            # ä¿å­˜çŠ¶æ€
            self.save_state(current_state)
            
        except Exception as e:
            print(f"âš ï¸  Failed to update batch status: {str(e)}")
    
    def wait_for_completion_threads(self, threads, state):
        """ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ"""
        try:
            completed = 0
            total = len(threads)
            
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()
                completed += 1
                print(f"Thread completed: {completed}/{total}")
            
            print(f"âœ… All batches processing completed")
            
        except Exception as e:
            print(f"âš ï¸  Error waiting for threads to complete: {str(e)}")
    
    def wait_for_completion(self, processes, state):
        """ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ"""
        try:
            completed = 0
            total = len(processes)
            
            while completed < total:
                time.sleep(5)
                
                # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                for i, process in enumerate(processes):
                    if process and process.is_alive():
                        continue
                    elif process:
                        # è¿›ç¨‹å·²ç»“æŸ
                        processes[i] = None
                        completed += 1
                        print(f"Process completed: {completed}/{total}")
            
            print(get_message_fallback("all_batches_completed_status"))
            
        except Exception as e:
            print(get_message_fallback("wait_process_error", error=str(e)))
    
    def merge_final_results(self, state=None):
        """åˆå¹¶æœ€ç»ˆç»“æœ"""
        try:
            self._ensure_config_loaded()
            
            # é‡æ–°åŠ è½½æœ€æ–°çš„çŠ¶æ€æ–‡ä»¶ï¼Œç¡®ä¿è·å–åˆ°æ‰€æœ‰æ‰¹æ¬¡çš„æœ€ç»ˆçŠ¶æ€
            if os.path.exists(self.state_file):
                print(get_message_fallback("reload_state_file", file=self.state_file))
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    latest_state = json.load(f)
            else:
                print(get_message_fallback("state_file_not_exist"))
                latest_state = state
            
            if not latest_state:
                raise ValueError(get_message_fallback("cannot_get_state"))
            
            # æ˜¾ç¤ºæ‰¹æ¬¡çŠ¶æ€ç»Ÿè®¡
            completed_count = sum(1 for b in latest_state['batches'] if b['status'] == 'completed')
            total_count = len(latest_state['batches'])
            print(get_message_fallback("batch_status_stats", completed=completed_count, total=total_count))
            
            from result_merger import ResultMerger
            
            output_directory = self.config['paths'].get(
                'output_directory', 
                os.path.dirname(self.config['paths']['input_xml_path'])
            )
            
            merger = ResultMerger(output_directory)
            results = merger.merge_batch_results(latest_state, backup_individual=True)
            
            if results.get('xml_merge', {}).get('success') or results.get('excel_merge', {}).get('success'):
                print("âœ… Result merge successful!")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if self.config['parallel_settings'].get('cleanup_temp_files', True):
                    self.cleanup_temp_files()
                
                return True
            else:
                print("âŒ Result merge failed")
                return False
                
        except Exception as e:
            print(f"âŒ Final result merge failed: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def save_state(self, state):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(get_message_fallback("save_state_failed", error=str(e)))
    
    def cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
            if os.path.exists(self.state_file):
                os.remove(self.state_file)
            print(get_message_fallback("temp_cleanup_success"))
        except Exception as e:
            print(get_message_fallback("temp_cleanup_error", error=str(e)))


def main():
    """ä¸»å‡½æ•°"""
    try:
        print("="*60)
        print(get_message_fallback("parallel_screening_system"))
        print("="*60)
        
        # åˆ›å»ºç®¡ç†å™¨
        manager = ParallelScreeningManager()
        
        # å¯åŠ¨å¹¶è¡Œç­›é€‰
        success = manager.start_parallel_screening()
        
        if success:
            print(f"\n{get_message_fallback('parallel_screening_completed')}")
            return 0
        else:
            print(f"\n{get_message_fallback('parallel_screening_failed')}")
            return 1
            
    except KeyboardInterrupt:
        print(f"\n\n{get_message_fallback('user_interrupted')}")
        return 1
    except Exception as e:
        print(f"\nâŒ System error: {str(e)}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())