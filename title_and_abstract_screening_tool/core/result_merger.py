#!/usr/bin/env python3
"""
ç»“æœåˆå¹¶å™¨
æŒ‰é¡ºåºåˆå¹¶å„ä¸ªå¹¶è¡Œç­›é€‰å™¨äº§ç”Ÿçš„XMLå’ŒExcelæ–‡ä»¶
"""

import os
import pandas as pd
import xml.etree.ElementTree as ET
from datetime import datetime
from pathlib import Path
import json
import shutil


class ResultMerger:
    """ç»“æœåˆå¹¶å™¨"""
    
    def __init__(self, output_directory, final_output_prefix="final_results"):
        self.output_directory = Path(output_directory)
        self.final_output_prefix = final_output_prefix
        self.output_directory.mkdir(parents=True, exist_ok=True)
        
    def merge_xml_files(self, xml_file_paths, output_xml_path):
        """
        åˆå¹¶å¤šä¸ªXMLæ–‡ä»¶ï¼Œä¿æŒè®°å½•é¡ºåº
        
        Args:
            xml_file_paths (list): XMLæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ŒæŒ‰æ‰¹æ¬¡é¡ºåºæ’åˆ—
            output_xml_path (str): è¾“å‡ºXMLæ–‡ä»¶è·¯å¾„
        """
        try:
            try:
                from i18n.i18n_manager import get_message
                print(f"\n{get_message('xml_merge_start')}")
                print(get_message("files_to_merge", count=len(xml_file_paths)))
            except:
                print(f"\nğŸ“„ Starting XML file merge...")
                print(f"Files to merge: {len(xml_file_paths)}")
            
            # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            missing_files = []
            for file_path in xml_file_paths:
                if not os.path.exists(file_path):
                    missing_files.append(file_path)
            
            if missing_files:
                raise FileNotFoundError(f"ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨: {missing_files}")
            
            # è§£æç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºåŸºç¡€ç»“æ„
            first_file = xml_file_paths[0]
            tree = ET.parse(first_file)
            root = tree.getroot()
            
            # è·å–recordså®¹å™¨
            records_container = root.find('.//records')
            if records_container is None:
                # å¦‚æœæ²¡æœ‰recordså®¹å™¨ï¼Œåˆ›å»ºä¸€ä¸ª
                records_container = ET.SubElement(root, 'records')
            else:
                # æ¸…ç©ºç°æœ‰è®°å½•
                records_container.clear()
            
            total_merged_records = 0
            
            # æŒ‰é¡ºåºåˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„è®°å½•
            for i, file_path in enumerate(xml_file_paths, 1):
                try:
                    from i18n.i18n_manager import get_message
                    print(get_message("processing_file", current=i, total=len(xml_file_paths), filename=os.path.basename(file_path)))
                except:
                    print(f"  Processing file {i}/{len(xml_file_paths)}: {os.path.basename(file_path)}")
                
                # è§£æå½“å‰æ–‡ä»¶
                current_tree = ET.parse(file_path)
                current_root = current_tree.getroot()
                
                # è·å–å½“å‰æ–‡ä»¶çš„records
                current_records = current_root.findall('.//record')
                
                # å°†è®°å½•æ·»åŠ åˆ°ä¸»æ–‡ä»¶
                for record in current_records:
                    records_container.append(record)
                    total_merged_records += 1
            
            # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
            ET.indent(tree, space="  ", level=0)  # æ ¼å¼åŒ–XML
            tree.write(output_xml_path, encoding='utf-8', xml_declaration=True)
            
            try:
                from i18n.i18n_manager import get_message
                print(get_message("xml_merge_completed", count=total_merged_records))
                print(get_message("xml_output_file", path=output_xml_path))
            except:
                print(f"âœ“ XML merge completed: {total_merged_records} records")
                print(f"âœ“ Output file: {output_xml_path}")
            
            return {
                'success': True,
                'total_records': total_merged_records,
                'output_file': output_xml_path
            }
            
        except Exception as e:
            try:
                from i18n.i18n_manager import get_message
                print(get_message("xml_merge_failed", error=str(e)))
            except:
                print(f"âŒ XML merge failed: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def merge_excel_files(self, excel_file_paths, output_excel_path):
        """
        åˆå¹¶å¤šä¸ªExcelæ–‡ä»¶ï¼Œä¿æŒè®°å½•é¡ºåº
        
        Args:
            excel_file_paths (list): Excelæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ŒæŒ‰æ‰¹æ¬¡é¡ºåºæ’åˆ—
            output_excel_path (str): è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
        """
        try:
            try:
                from i18n.i18n_manager import get_message
                print(f"\n{get_message('excel_merge_start')}")
                print(get_message("files_to_merge", count=len(excel_file_paths)))
            except:
                print(f"\nğŸ“Š Starting Excel file merge...")
                print(f"Files to merge: {len(excel_file_paths)}")
            
            # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            missing_files = []
            for file_path in excel_file_paths:
                if not os.path.exists(file_path):
                    missing_files.append(file_path)
            
            if missing_files:
                raise FileNotFoundError(f"ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨: {missing_files}")
            
            merged_dataframes = []
            total_merged_records = 0
            
            # æŒ‰é¡ºåºè¯»å–å¹¶åˆå¹¶æ‰€æœ‰Excelæ–‡ä»¶
            for i, file_path in enumerate(excel_file_paths, 1):
                try:
                    from i18n.i18n_manager import get_message
                    print(get_message("excel_processing_file", current=i, total=len(excel_file_paths), filename=os.path.basename(file_path)))
                except:
                    print(f"  Processing file {i}/{len(excel_file_paths)}: {os.path.basename(file_path)}")
                
                try:
                    # è¯»å–Excelæ–‡ä»¶
                    df = pd.read_excel(file_path)
                    
                    if not df.empty:
                        merged_dataframes.append(df)
                        total_merged_records += len(df)
                        print(f"    - è¯»å– {len(df)} æ¡è®°å½•")
                    else:
                        print(f"    - æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                        
                except Exception as e:
                    print(f"    âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
                    continue
            
            if not merged_dataframes:
                raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„Excelæ–‡ä»¶å¯ä»¥åˆå¹¶")
            
            # åˆå¹¶æ‰€æœ‰DataFrame
            final_df = pd.concat(merged_dataframes, ignore_index=True)
            
            # æ·»åŠ æ‰¹æ¬¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            if 'batch_id' not in final_df.columns:
                batch_info = []
                current_batch = 1
                current_count = 0
                
                for df in merged_dataframes:
                    batch_size = len(df)
                    batch_info.extend([current_batch] * batch_size)
                    current_batch += 1
                
                final_df['batch_id'] = batch_info
            
            # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
            final_df.to_excel(output_excel_path, index=False)
            
            print(f"âœ“ Excelåˆå¹¶å®Œæˆ: {total_merged_records} æ¡è®°å½•")
            print(f"âœ“ è¾“å‡ºæ–‡ä»¶: {output_excel_path}")
            
            return {
                'success': True,
                'total_records': total_merged_records,
                'output_file': output_excel_path,
                'columns': list(final_df.columns)
            }
            
        except Exception as e:
            print(f"âŒ Excelåˆå¹¶å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def merge_batch_results(self, state, backup_individual=True):
        """
        æ ¹æ®çŠ¶æ€æ–‡ä»¶åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        
        Args:
            state (dict): åŒ…å«æ‰¹æ¬¡ä¿¡æ¯çš„çŠ¶æ€å­—å…¸
            backup_individual (bool): æ˜¯å¦å¤‡ä»½ä¸ªåˆ«ç»“æœæ–‡ä»¶
        """
        try:
            print(f"\nğŸ”„ å¼€å§‹åˆå¹¶æ‰¹æ¬¡ç»“æœ...")
            
            # è·å–å·²å®Œæˆçš„æ‰¹æ¬¡
            completed_batches = [b for b in state['batches'] if b['status'] == 'completed']
            
            if not completed_batches:
                raise ValueError("æ²¡æœ‰å·²å®Œæˆçš„æ‰¹æ¬¡å¯ä»¥åˆå¹¶")
            
            # æŒ‰batch_idæ’åºç¡®ä¿é¡ºåºæ­£ç¡®
            completed_batches.sort(key=lambda x: x['batch_id'])
            
            print(f"å‘ç° {len(completed_batches)} ä¸ªå·²å®Œæˆæ‰¹æ¬¡")
            
            # æ”¶é›†æ–‡ä»¶è·¯å¾„
            xml_files = []
            excel_files = []
            
            for batch in completed_batches:
                if 'output_files' in batch:
                    xml_file = batch['output_files'].get('xml')
                    excel_file = batch['output_files'].get('excel')
                    
                    if xml_file and os.path.exists(xml_file):
                        xml_files.append(xml_file)
                    
                    if excel_file and os.path.exists(excel_file):
                        excel_files.append(excel_file)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            output_xml = self.output_directory / f"{self.final_output_prefix}_{timestamp}.xml"
            output_excel = self.output_directory / f"{self.final_output_prefix}_{timestamp}.xlsx"
            
            results = {
                'timestamp': timestamp,
                'session_id': state.get('session_id', 'unknown'),
                'total_batches': len(completed_batches),
                'xml_merge': None,
                'excel_merge': None
            }
            
            # åˆå¹¶XMLæ–‡ä»¶
            if xml_files:
                print(f"\nğŸ“„ åˆå¹¶ {len(xml_files)} ä¸ªXMLæ–‡ä»¶...")
                results['xml_merge'] = self.merge_xml_files(xml_files, str(output_xml))
            else:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°XMLæ–‡ä»¶")
            
            # åˆå¹¶Excelæ–‡ä»¶
            if excel_files:
                print(f"\nğŸ“Š åˆå¹¶ {len(excel_files)} ä¸ªExcelæ–‡ä»¶...")
                results['excel_merge'] = self.merge_excel_files(excel_files, str(output_excel))
            else:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°Excelæ–‡ä»¶")
            
            # ğŸ’° æ–°å¢ï¼šåˆå¹¶tokenç»Ÿè®¡å’Œæˆæœ¬åˆ†æ
            print(f"\nğŸ“ˆ åˆå¹¶tokenç»Ÿè®¡å’Œæˆæœ¬åˆ†æ...")
            results['token_merge'] = self.merge_token_statistics(state, timestamp)
            
            # å¤‡ä»½ä¸ªåˆ«ç»“æœæ–‡ä»¶
            if backup_individual:
                self.backup_individual_files(xml_files + excel_files, timestamp)
            
            # ç”Ÿæˆåˆå¹¶æŠ¥å‘Š
            self.generate_merge_report(results, completed_batches)
            
            return results
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ç»“æœåˆå¹¶å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def backup_individual_files(self, file_paths, timestamp):
        """å¤‡ä»½ä¸ªåˆ«ç»“æœæ–‡ä»¶"""
        try:
            backup_dir = self.output_directory / f"backup_{timestamp}"
            backup_dir.mkdir(exist_ok=True)
            
            print(f"\nğŸ’¾ å¤‡ä»½ä¸ªåˆ«ç»“æœæ–‡ä»¶åˆ°: {backup_dir}")
            
            for file_path in file_paths:
                if os.path.exists(file_path):
                    filename = os.path.basename(file_path)
                    backup_path = backup_dir / filename
                    shutil.copy2(file_path, backup_path)
                    print(f"  âœ“ {filename}")
            
            print(f"âœ“ å¤‡ä»½å®Œæˆ: {len(file_paths)} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            print(f"âš ï¸  å¤‡ä»½å¤±è´¥: {str(e)}")
    
    def generate_merge_report(self, results, completed_batches):
        """ç”Ÿæˆåˆå¹¶æŠ¥å‘Š"""
        try:
            timestamp = results['timestamp']
            report_path = self.output_directory / f"merge_report_{timestamp}.txt"
            
            report_lines = []
            report_lines.append("=" * 80)
            report_lines.append("å¹¶è¡Œç­›é€‰ç»“æœåˆå¹¶æŠ¥å‘Š")
            report_lines.append("=" * 80)
            report_lines.append("")
            
            # åŸºæœ¬ä¿¡æ¯
            report_lines.append("åŸºæœ¬ä¿¡æ¯:")
            report_lines.append(f"  ä¼šè¯ID: {results['session_id']}")
            report_lines.append(f"  åˆå¹¶æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_lines.append(f"  åˆå¹¶æ‰¹æ¬¡æ•°: {results['total_batches']}")
            report_lines.append("")
            
            # XMLåˆå¹¶ç»“æœ
            if results['xml_merge']:
                xml_result = results['xml_merge']
                if xml_result['success']:
                    report_lines.append("XMLåˆå¹¶ç»“æœ:")
                    report_lines.append(f"  çŠ¶æ€: âœ… æˆåŠŸ")
                    report_lines.append(f"  æ€»è®°å½•æ•°: {xml_result['total_records']}")
                    report_lines.append(f"  è¾“å‡ºæ–‡ä»¶: {os.path.basename(xml_result['output_file'])}")
                else:
                    report_lines.append("XMLåˆå¹¶ç»“æœ:")
                    report_lines.append(f"  çŠ¶æ€: âŒ å¤±è´¥")
                    report_lines.append(f"  é”™è¯¯: {xml_result['error']}")
                report_lines.append("")
            
            # Excelåˆå¹¶ç»“æœ
            if results['excel_merge']:
                excel_result = results['excel_merge']
                if excel_result['success']:
                    report_lines.append("Excelåˆå¹¶ç»“æœ:")
                    report_lines.append(f"  çŠ¶æ€: âœ… æˆåŠŸ")
                    report_lines.append(f"  æ€»è®°å½•æ•°: {excel_result['total_records']}")
                    report_lines.append(f"  è¾“å‡ºæ–‡ä»¶: {os.path.basename(excel_result['output_file'])}")
                    report_lines.append(f"  åŒ…å«åˆ—: {', '.join(excel_result['columns'])}")
                else:
                    report_lines.append("Excelåˆå¹¶ç»“æœ:")
                    report_lines.append(f"  çŠ¶æ€: âŒ å¤±è´¥")
                    report_lines.append(f"  é”™è¯¯: {excel_result['error']}")
                report_lines.append("")
            
            # ğŸ’° æ–°å¢ï¼šTokenæˆæœ¬ç»Ÿè®¡
            if results.get('token_merge'):
                token_result = results['token_merge']
                if token_result.get('success', False):
                    report_lines.append("Tokenæˆæœ¬ç»Ÿè®¡:")
                    report_lines.append(f"  çŠ¶æ€: âœ… æˆåŠŸ")
                    report_lines.append(f"  Tokenè®°å½•æ•°: {token_result['total_records']}")
                    report_lines.append(f"  åˆå¹¶CSV: {os.path.basename(token_result['merged_csv'])}")
                    
                    # æ˜¾ç¤ºæˆæœ¬ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if 'usd_analysis' in token_result:
                        usd_analysis = token_result['usd_analysis']
                        cny_analysis = token_result['cny_analysis']
                        
                        report_lines.append(f"  æ€»tokenæ•°: {usd_analysis['summary']['total_tokens']:,}")
                        report_lines.append(f"  APIè°ƒç”¨æ¬¡æ•°: {usd_analysis['summary']['total_calls']:,}")
                        report_lines.append(f"  ç¾å…ƒæˆæœ¬: ${usd_analysis['total_cost']:.4f} USD")
                        report_lines.append(f"  äººæ°‘å¸æˆæœ¬: Â¥{cny_analysis['total_cost']:.2f} CNY")
                        
                        # æŒ‰æ¨¡å‹æˆæœ¬ç»Ÿè®¡
                        report_lines.append("  æŒ‰æ¨¡å‹æˆæœ¬:")
                        for model_name, model_data in usd_analysis['by_model'].items():
                            cny_cost = cny_analysis['by_model'][model_name]['total_cost']
                            report_lines.append(f"    - {model_name}: ${model_data['total_cost']:.4f} USD / Â¥{cny_cost:.2f} CNY")
                    
                    if 'cost_error' in token_result:
                        report_lines.append(f"  æˆæœ¬è®¡ç®—é”™è¯¯: {token_result['cost_error']}")
                        
                else:
                    report_lines.append("Tokenæˆæœ¬ç»Ÿè®¡:")
                    report_lines.append(f"  çŠ¶æ€: âŒ å¤±è´¥")
                    report_lines.append(f"  é”™è¯¯: {token_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                report_lines.append("")
            
            # æ‰¹æ¬¡è¯¦æƒ…
            report_lines.append("æ‰¹æ¬¡è¯¦æƒ…:")
            report_lines.append("-" * 80)
            header = f"{'æ‰¹æ¬¡ID':<8} {'èµ·å§‹è®°å½•':<10} {'ç»“æŸè®°å½•':<10} {'è®°å½•æ•°':<8} {'çŠ¶æ€':<10}"
            report_lines.append(header)
            report_lines.append("-" * 80)
            
            for batch in completed_batches:
                row = (f"{batch['batch_id']:<8} "
                      f"{batch['start_record']:<10} "
                      f"{batch['end_record']:<10} "
                      f"{batch['record_count']:<8} "
                      f"{batch['status']:<10}")
                report_lines.append(row)
            
            report_lines.append("-" * 80)
            report_lines.append("")
            report_lines.append("=" * 80)
            
            # ä¿å­˜æŠ¥å‘Š
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            
            print(f"\nğŸ“‹ åˆå¹¶æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆåˆå¹¶æŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def merge_token_statistics(self, state, timestamp):
        """
        åˆå¹¶å„æ‰¹æ¬¡çš„tokenç»Ÿè®¡å’Œæˆæœ¬åˆ†æ
        
        Args:
            state (dict): çŠ¶æ€å­—å…¸
            timestamp (str): æ—¶é—´æˆ³
        """
        try:
            print(f"\nğŸ“Š å¼€å§‹åˆå¹¶tokenç»Ÿè®¡...")
            
            # è·å–å·²å®Œæˆçš„æ‰¹æ¬¡
            completed_batches = [b for b in state['batches'] if b['status'] == 'completed']
            
            if not completed_batches:
                print("âš ï¸  æ²¡æœ‰å·²å®Œæˆæ‰¹æ¬¡çš„tokenç»Ÿè®¡å¯ä»¥åˆå¹¶")
                return None
            
            # æ”¶é›†æ‰€æœ‰token CSVæ–‡ä»¶
            token_csv_files = []
            all_tokens_log = []
            
            for batch in completed_batches:
                if 'output_files' in batch:
                    # å°è¯•æŸ¥æ‰¾token usage CSVæ–‡ä»¶
                    xml_file = batch['output_files'].get('xml', '')
                    if xml_file:
                        # ä»XMLæ–‡ä»¶è·¯å¾„æ¨å¯¼token CSVæ–‡ä»¶è·¯å¾„
                        base_name = os.path.splitext(xml_file)[0]
                        token_csv = f"{base_name}_tokens_usage.csv"
                        
                        if os.path.exists(token_csv):
                            token_csv_files.append(token_csv)
                            print(f"  æ‰¾åˆ°æ‰¹æ¬¡ {batch['batch_id']} çš„tokenç»Ÿè®¡: {os.path.basename(token_csv)}")
                            
                            # è¯»å–CSVæ•°æ®
                            try:
                                import pandas as pd
                                df = pd.read_csv(token_csv)
                                batch_tokens = df.to_dict('records')
                                all_tokens_log.extend(batch_tokens)
                            except Exception as e:
                                print(f"    âš ï¸  è¯»å–å¤±è´¥: {str(e)}")
                        else:
                            print(f"  æ‰¹æ¬¡ {batch['batch_id']} æ²¡æœ‰æ‰¾åˆ°tokenç»Ÿè®¡æ–‡ä»¶")
            
            if not all_tokens_log:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„tokenç»Ÿè®¡æ•°æ®")
                return None
            
            print(f"ğŸ“‹ æ€»è®¡æ”¶é›†åˆ° {len(all_tokens_log)} æ¡tokenä½¿ç”¨è®°å½•")
            
            # ä¿å­˜åˆå¹¶åçš„tokenç»Ÿè®¡CSV
            merged_csv_path = self.output_directory / f"final_tokens_usage_{timestamp}.csv"
            
            import pandas as pd
            merged_df = pd.DataFrame(all_tokens_log)
            merged_df.to_csv(merged_csv_path, index=False)
            print(f"âœ“ åˆå¹¶tokenç»Ÿè®¡CSV: {merged_csv_path}")
            
            # è®¡ç®—å¹¶ä¿å­˜æˆæœ¬åˆ†æ
            try:
                from src.token_cost_calculator import TokenCostCalculator
                
                calculator = TokenCostCalculator()
                
                # è®¡ç®—ç¾å…ƒå’Œäººæ°‘å¸æˆæœ¬
                usd_analysis = calculator.calculate_tokens_log_costs(all_tokens_log, "USD")
                cny_analysis = calculator.calculate_tokens_log_costs(all_tokens_log, "CNY")
                
                # ä¿å­˜æˆæœ¬æŠ¥å‘Š
                base_path = str(self.output_directory / f"final_cost_analysis_{timestamp}")
                calculator.save_cost_report(usd_analysis, f"{base_path}_usd")
                calculator.save_cost_report(cny_analysis, f"{base_path}_cny")
                
                # æ˜¾ç¤ºæˆæœ¬æ±‡æ€»
                print(f"\nğŸ’° æœ€ç»ˆæˆæœ¬æ±‡æ€»:")
                print(f"  ç¾å…ƒè´¹ç”¨: ${usd_analysis['total_cost']:.4f} USD")
                print(f"  äººæ°‘å¸è´¹ç”¨: Â¥{cny_analysis['total_cost']:.2f} CNY")
                print(f"  æ€»tokens: {usd_analysis['summary']['total_tokens']:,}")
                print(f"  APIè°ƒç”¨æ¬¡æ•°: {usd_analysis['summary']['total_calls']:,}")
                
                # æŒ‰æ¨¡å‹æ˜¾ç¤ºæˆæœ¬
                print(f"\nğŸ·ï¸  å„æ¨¡å‹æˆæœ¬ç»Ÿè®¡:")
                for model_name, model_data in usd_analysis['by_model'].items():
                    cny_cost = cny_analysis['by_model'][model_name]['total_cost']
                    print(f"  - {model_name}: ${model_data['total_cost']:.4f} USD / Â¥{cny_cost:.2f} CNY")
                
                return {
                    'success': True,
                    'merged_csv': str(merged_csv_path),
                    'usd_analysis': usd_analysis,
                    'cny_analysis': cny_analysis,
                    'total_records': len(all_tokens_log)
                }
                
            except ImportError:
                print("âš ï¸  æˆæœ¬è®¡ç®—æ¨¡å—æœªæ‰¾åˆ°ï¼Œä»…ä¿å­˜åŸºç¡€tokenç»Ÿè®¡")
                return {
                    'success': True,
                    'merged_csv': str(merged_csv_path),
                    'total_records': len(all_tokens_log)
                }
                
            except Exception as e:
                print(f"âš ï¸  æˆæœ¬è®¡ç®—å¤±è´¥: {str(e)}")
                return {
                    'success': True,
                    'merged_csv': str(merged_csv_path),
                    'total_records': len(all_tokens_log),
                    'cost_error': str(e)
                }
            
        except Exception as e:
            print(f"âŒ tokenç»Ÿè®¡åˆå¹¶å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def cleanup_temp_files(self, temp_files):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            print(f"\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            
            cleaned_count = 0
            for file_path in temp_files:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    cleaned_count += 1
                    print(f"  âœ“ åˆ é™¤: {os.path.basename(file_path)}")
            
            print(f"âœ“ æ¸…ç†å®Œæˆ: {cleaned_count} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")


def merge_results_from_state(state_file_path, output_directory, 
                           final_output_prefix="final_results", 
                           backup_individual=True):
    """
    ä»çŠ¶æ€æ–‡ä»¶åˆå¹¶ç»“æœçš„ä¾¿æ·å‡½æ•°
    
    Args:
        state_file_path (str): çŠ¶æ€æ–‡ä»¶è·¯å¾„
        output_directory (str): è¾“å‡ºç›®å½•
        final_output_prefix (str): æœ€ç»ˆæ–‡ä»¶å‰ç¼€
        backup_individual (bool): æ˜¯å¦å¤‡ä»½ä¸ªåˆ«æ–‡ä»¶
    """
    try:
        # è¯»å–çŠ¶æ€æ–‡ä»¶
        with open(state_file_path, 'r', encoding='utf-8') as f:
            state = json.load(f)
        
        # åˆ›å»ºåˆå¹¶å™¨
        merger = ResultMerger(output_directory, final_output_prefix)
        
        # æ‰§è¡Œåˆå¹¶
        results = merger.merge_batch_results(state, backup_individual)
        
        return results
        
    except Exception as e:
        print(f"âŒ ä»çŠ¶æ€æ–‡ä»¶åˆå¹¶å¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python result_merger.py <çŠ¶æ€æ–‡ä»¶> <è¾“å‡ºç›®å½•>")
        sys.exit(1)
    
    state_file = sys.argv[1]
    output_dir = sys.argv[2]
    
    print("ğŸ”„ å¼€å§‹ç»“æœåˆå¹¶...")
    results = merge_results_from_state(state_file, output_dir)
    
    if results.get('success', False):
        print("âœ… åˆå¹¶æˆåŠŸ!")
    else:
        print(f"âŒ åˆå¹¶å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
        sys.exit(1)
