#!/usr/bin/env python3
"""
å¹¶è¡Œæ•°æ®æå–æ§åˆ¶å™¨
Parallel Data Extraction Controller

è´Ÿè´£åè°ƒå¤šä¸ªå·¥ä½œå™¨å¹¶è¡Œæå–æ•°æ®
Coordinates multiple workers for parallel data extraction
"""

import os
import json
import time
import psutil
import multiprocessing as mp
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed

try:
    from i18n.i18n_manager import get_message
except ImportError:
    def get_message(key, **kwargs):
        return key.format(**kwargs) if kwargs else key

try:
    from .progress_monitor import ProgressMonitor
    from .result_merger import ResultMerger
    from .resource_detector import ResourceDetector
except ImportError:
    from progress_monitor import ProgressMonitor
    from result_merger import ResultMerger
    from resource_detector import ResourceDetector


def process_batch_worker(batch_id: int, document_batch: List[str], config_path: str, temp_dir: str) -> Dict[str, Any]:
    """ç‹¬ç«‹çš„æ‰¹æ¬¡å¤„ç†å·¥ä½œå‡½æ•°ï¼Œé¿å…picklingé—®é¢˜"""
    try:
        from src.main import DataExtractor
        
        # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºç‹¬ç«‹çš„æå–å™¨
        extractor = DataExtractor(config_path)
        
        # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰æ–‡æ¡£
        results = []
        for doc_path in document_batch:
            try:
                result = extractor.process_single_document(doc_path)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"Error processing {doc_path}: {e}")
        
        # ä¿å­˜æ‰¹æ¬¡ç»“æœ
        if results:
            batch_output_path = os.path.join(temp_dir, f"batch_{batch_id}_results.xlsx")
            extractor.save_batch_results(results, batch_output_path)
        
        return {
            "success": True,
            "batch_id": batch_id,
            "processed_count": len(results),
            "total_count": len(document_batch)
        }
        
    except Exception as e:
        return {
            "success": False,
            "batch_id": batch_id,
            "error": str(e)
        }


class ParallelExtractionManager:
    """å¹¶è¡Œæå–ç®¡ç†å™¨ / Parallel Extraction Manager"""
    
    def __init__(self, config_path: str):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        self.config_path = config_path
        self.config = self.load_config()
        self.resource_detector = ResourceDetector()
        self.progress_monitor = ProgressMonitor()
        self.result_merger = ResultMerger()
        self.state_file = None
        self.temp_dir = None
        
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise Exception(get_message("config_error", error=str(e)))
    
    def validate_and_optimize_config(self) -> Tuple[bool, List[str]]:
        """éªŒè¯å¹¶ä¼˜åŒ–é…ç½®"""
        warnings = []
        
        # æ£€æµ‹ç³»ç»Ÿèµ„æº
        cpu_cores = self.resource_detector.get_cpu_cores()
        available_memory = self.resource_detector.get_available_memory()
        
        # è·å–é…ç½®çš„å·¥ä½œå™¨æ•°é‡
        requested_workers = self.config.get("parallel_settings", {}).get("parallel_workers", 1)
        safe_cpu_cores = max(1, cpu_cores - 2)  # ä¿ç•™2ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
        
        # CPUæ ¸å¿ƒæ£€æŸ¥
        if requested_workers > safe_cpu_cores:
            warnings.append(get_message("cpu_warning", 
                                      requested=requested_workers, 
                                      safe=safe_cpu_cores))
            warnings.append(get_message("cpu_suggestion", 
                                      recommended=safe_cpu_cores))
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—
        memory_per_worker = self.config.get("resource_management", {}).get("memory_limit_mb", 2048) / 1024
        estimated_memory = requested_workers * memory_per_worker
        
        if estimated_memory > available_memory * 0.8:  # ä½¿ç”¨80%å†…å­˜é˜ˆå€¼
            recommended_workers = max(1, int(available_memory * 0.8 / memory_per_worker))
            warnings.append(get_message("memory_warning",
                                      estimated=estimated_memory,
                                      available=available_memory))
            warnings.append(get_message("memory_suggestion",
                                      recommended=recommended_workers))
        
        return len(warnings) == 0, warnings
    
    def setup_extraction_environment(self) -> str:
        """è®¾ç½®æå–ç¯å¢ƒ"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.temp_dir = self.config.get("parallel_settings", {}).get("temp_dir", "temp_parallel")
        self.temp_dir = f"{self.temp_dir}_{timestamp}"
        
        Path(self.temp_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
        self.state_file = f"extraction_state_{timestamp}.json"
        
        return self.temp_dir
    
    def scan_input_documents(self) -> List[str]:
        """æ‰«æè¾“å…¥æ–‡æ¡£"""
        input_folder = self.config["paths"]["input_folder"]
        if not os.path.exists(input_folder):
            raise Exception(get_message("file_not_found", file=input_folder))
        
        # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
        supported_extensions = ['.pdf', '.docx', '.doc', '.txt']
        documents = []
        
        for ext in supported_extensions:
            # ä½¿ç”¨é€’å½’æ‰«æï¼Œè¿™æ ·ä¼šåŒ…å«å½“å‰ç›®å½•å’Œå­ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
            documents.extend(Path(input_folder).glob(f"**/*{ext}"))
        
        # å»é‡å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„
        unique_documents = list(set(str(doc) for doc in documents))
        return sorted(unique_documents)
    
    def distribute_documents(self, documents: List[str], num_workers: int) -> List[List[str]]:
        """åˆ†å‘æ–‡æ¡£åˆ°å„ä¸ªå·¥ä½œå™¨"""
        if not documents:
            return []
        
        # å¹³å‡åˆ†é…æ–‡æ¡£
        docs_per_worker = len(documents) // num_workers
        remainder = len(documents) % num_workers
        
        batches = []
        start_idx = 0
        
        for i in range(num_workers):
            # å¦‚æœæœ‰ä½™æ•°ï¼Œå‰å‡ ä¸ªå·¥ä½œå™¨å¤šåˆ†é…ä¸€ä¸ªæ–‡æ¡£
            batch_size = docs_per_worker + (1 if i < remainder else 0)
            end_idx = start_idx + batch_size
            
            if start_idx < len(documents):
                batches.append(documents[start_idx:end_idx])
                start_idx = end_idx
        
        return [batch for batch in batches if batch]  # è¿‡æ»¤ç©ºæ‰¹æ¬¡
    
    def save_state(self, state_data: Dict[str, Any]):
        """ä¿å­˜çŠ¶æ€"""
        if self.state_file:
            try:
                with open(self.state_file, 'w', encoding='utf-8') as f:
                    json.dump(state_data, f, indent=2, ensure_ascii=False)
            except Exception as e:
                print(f"Warning: Failed to save state: {e}")
    
    def start_parallel_extraction(self) -> bool:
        """å¯åŠ¨å¹¶è¡Œæå–"""
        try:
            print(get_message("starting_new_task"))
            
            # éªŒè¯é…ç½®
            is_valid, warnings = self.validate_and_optimize_config()
            if warnings:
                print(get_message("resource_warning"))
                for warning in warnings:
                    print(warning)
                print()
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                choice = input("Continue with current settings? (y/n): ").lower().strip()
                if choice != 'y':
                    return False
            
            # è®¾ç½®ç¯å¢ƒ
            temp_dir = self.setup_extraction_environment()
            print(get_message("temp_prepared", temp_dir=temp_dir))
            
            # æ‰«ææ–‡æ¡£
            documents = self.scan_input_documents()
            if not documents:
                print(get_message("no_documents"))
                return False
            
            print(get_message("documents_found", count=len(documents)))
            
            # è·å–å·¥ä½œå™¨æ•°é‡
            num_workers = self.config.get("parallel_settings", {}).get("parallel_workers", 1)
            print(get_message("parallel_setup", workers=num_workers))
            
            # åˆ†å‘æ–‡æ¡£
            document_batches = self.distribute_documents(documents, num_workers)
            
            # åˆå§‹åŒ–çŠ¶æ€
            state_data = {
                "timestamp": datetime.now().isoformat(),
                "total_documents": len(documents),
                "total_batches": len(document_batches),
                "completed_batches": 0,
                "failed_batches": [],
                "temp_dir": temp_dir,
                "status": "running"
            }
            self.save_state(state_data)
            
            # å¯åŠ¨å¹¶è¡Œå¤„ç†
            return self._execute_parallel_extraction(document_batches, state_data)
            
        except Exception as e:
            print(get_message("system_error", error=str(e)))
            return False
    
    def _execute_parallel_extraction(self, document_batches: List[List[str]], state_data: Dict[str, Any]) -> bool:
        """æ‰§è¡Œå¹¶è¡Œæå–"""
        try:
            from src.main import DataExtractor
            
            # å¯åŠ¨è¿›åº¦ç›‘æ§
            self.progress_monitor.start_monitoring(len(document_batches))
            
            completed_batches = 0
            failed_batches = []
            
            # ä½¿ç”¨è¿›ç¨‹æ± æ‰§è¡Œå¹¶è¡Œæå–
            max_workers = min(len(document_batches), self.config.get("parallel_settings", {}).get("parallel_workers", 1))
            
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
                future_to_batch = {}
                for i, batch in enumerate(document_batches):
                    future = executor.submit(process_batch_worker, i, batch, self.config_path, self.temp_dir)
                    future_to_batch[future] = i
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_batch):
                    batch_id = future_to_batch[future]
                    try:
                        result = future.result()
                        if result["success"]:
                            completed_batches += 1
                            print(get_message("batch_completed", batch=batch_id + 1))
                        else:
                            failed_batches.append(batch_id)
                            print(get_message("batch_failed", 
                                            batch=batch_id + 1, 
                                            error=result.get("error", "Unknown error")))
                        
                        # æ›´æ–°è¿›åº¦
                        progress = (completed_batches + len(failed_batches)) / len(document_batches) * 100
                        print(get_message("extraction_progress",
                                        processed=completed_batches + len(failed_batches),
                                        total=len(document_batches),
                                        percentage=progress))
                        
                    except Exception as e:
                        failed_batches.append(batch_id)
                        print(get_message("batch_failed", 
                                        batch=batch_id + 1, 
                                        error=str(e)))
            
            # åœæ­¢è¿›åº¦ç›‘æ§
            self.progress_monitor.stop_monitoring()
            
            # åˆå¹¶ç»“æœ
            if completed_batches > 0:
                print(get_message("merging_results"))
                output_path = self.result_merger.merge_batch_results(self.temp_dir, self.config)
                print(get_message("saving_results", path=output_path))
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if self.config.get("parallel_settings", {}).get("cleanup_temp_files", True):
                print(get_message("cleanup_temp"))
                self._cleanup_temp_files()
            
            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
            state_data.update({
                "completed_batches": completed_batches,
                "failed_batches": failed_batches,
                "status": "completed" if not failed_batches else "completed_with_errors"
            })
            self.save_state(state_data)
            
            return completed_batches > 0
            
        except Exception as e:
            print(get_message("system_error", error=str(e)))
            return False
    
    def _process_batch(self, batch_id: int, document_batch: List[str]) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨éœ€è¦ä½œä¸ºç‹¬ç«‹å‡½æ•°æ¥é¿å…picklingé—®é¢˜
        return process_batch_worker(batch_id, document_batch, self.config_path, self.temp_dir)
    
    def resume_from_state(self, state_file_path: str) -> bool:
        """ä»çŠ¶æ€æ–‡ä»¶æ¢å¤ä»»åŠ¡ / Resume task from state file"""
        try:
            print(f"ğŸ”„ Loading state from: {state_file_path}")
            
            # è¯»å–çŠ¶æ€æ–‡ä»¶
            with open(state_file_path, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            
            # æ£€æŸ¥çŠ¶æ€
            if state_data.get("status") == "completed":
                print("âœ… Task already completed")
                return True
            
            # æ¢å¤é…ç½®
            self.temp_dir = state_data.get("temp_dir")
            completed_batches = state_data.get("completed_batches", 0)
            failed_batches = state_data.get("failed_batches", [])
            total_batches = state_data.get("total_batches", 0)
            
            print(f"ğŸ“Š Resume status: {completed_batches}/{total_batches} batches completed")
            
            if failed_batches:
                print(f"ğŸ”„ Retrying {len(failed_batches)} failed batches")
                # é‡æ–°å¤„ç†å¤±è´¥çš„æ‰¹æ¬¡
                # è¿™é‡Œéœ€è¦é‡æ–°æ„å»ºæ–‡æ¡£æ‰¹æ¬¡å¹¶å¤„ç†
                return self._retry_failed_batches(failed_batches, state_data)
            else:
                print("âœ… All batches completed successfully")
                return True
                
        except Exception as e:
            print(get_message("system_error", error=str(e)))
            return False
    
    def _retry_failed_batches(self, failed_batch_ids: List[int], state_data: Dict[str, Any]) -> bool:
        """é‡è¯•å¤±è´¥çš„æ‰¹æ¬¡ / Retry failed batches"""
        try:
            # è¿™é‡Œéœ€è¦é‡æ–°æ„å»ºå¤±è´¥æ‰¹æ¬¡çš„æ–‡æ¡£åˆ—è¡¨
            # ç”±äºåŸå§‹æ–‡æ¡£åˆ†é…ä¿¡æ¯å¯èƒ½ä¸¢å¤±ï¼Œæˆ‘ä»¬é‡æ–°æ‰«æå¹¶åˆ†é…
            documents = self.scan_input_documents()
            num_workers = self.config.get("parallel_settings", {}).get("parallel_workers", 1)
            document_batches = self.distribute_documents(documents, num_workers)
            
            # åªå¤„ç†å¤±è´¥çš„æ‰¹æ¬¡
            failed_batches = [document_batches[i] for i in failed_batch_ids if i < len(document_batches)]
            
            if not failed_batches:
                print("âœ… No failed batches to retry")
                return True
            
            # æ‰§è¡Œé‡è¯•
            return self._execute_parallel_extraction(failed_batches, state_data)
            
        except Exception as e:
            print(get_message("system_error", error=str(e)))
            return False
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ / Cleanup temporary files"""
        try:
            if self.temp_dir and os.path.exists(self.temp_dir):
                import shutil
                shutil.rmtree(self.temp_dir)
                print(f"âœ“ Cleaned up temporary directory: {self.temp_dir}")
        except Exception as e:
            print(f"Warning: Failed to cleanup temp files: {e}")