# SmartEBM ROB Assessment Tool v2.0 - Complete Documentation

Welcome to the comprehensive documentation for the SmartEBM ROB Assessment Tool v2.0. This guide provides detailed instructions, configuration options, and best practices for effective systematic review bias assessment.

## Table of Contents

1. [Project Background](#1-project-background)
2. [Advanced Features & Capabilities](#2-advanced-features--capabilities)
3. [Installation & Setup](#3-installation--setup)
4. [Configuration Guide](#4-configuration-guide)
5. [Usage Guide](#5-usage-guide)
6. [Output & Results](#6-output--results)
7. [Best Practices & Recommendations](#7-best-practices--recommendations)
8. [Troubleshooting](#8-troubleshooting)
9. [Project Structure](#9-project-structure)
10. [Advanced Features](#10-advanced-features)
11. [API Reference](#11-api-reference)
12. [License & Citation](#12-license--citation)

## 1. Project Background

Systematic reviews and network meta-analyses are cornerstones of evidence-based medicine. However, their production is resource-intensive, with the Risk of Bias (RoB) assessment phaseâ€”evaluating the methodological quality of included studiesâ€”being particularly labor-intensive and prone to subjectivity.

The SmartEBM Risk of Bias Assessment Tool addresses these challenges through advanced integration with the ROBUST-RCT framework, achieving 97.5% accuracy for high-confidence assessments across 3,276 evaluations in the SmartNMA research validation. The tool employs innovative two-step decomposition methodology that breaks down complex bias assessment into manageable components, combined with confidence-based classification that automatically identifies which assessments can be trusted versus those requiring manual review.

### ROBUST-RCT Framework Integration

The ROBUST-RCT (Risk Of Bias Using Structured Thinking - Randomized Controlled Trials) framework provides a systematic approach to bias assessment that our tool implements through:

**Two-Step Decomposition Process:**
1. **Domain Analysis**: Each bias domain (randomization, allocation concealment, blinding, etc.) is evaluated independently with domain-specific prompts and criteria
2. **Synthesis Integration**: Individual domain assessments are synthesized into overall study-level bias evaluation with confidence scoring

**Confidence-Based Classification:**
- **High Confidence (97.5% accuracy)**: Clear evidence and consistent LLM agreement across domains
- **Medium Confidence**: Minor discrepancies requiring targeted review
- **Low Confidence**: Significant conflicts or insufficient information requiring manual assessment

This approach enables researchers to focus their limited time on genuinely uncertain cases while trusting automated assessments for clear-cut evaluations.

### The SmartEBM Agent Ecosystem

While this repository focuses on the RoB Assessment Agent, it is part of a larger ecosystem of intelligent agents being developed by our team, which also includes:

- **Title and Abstract Screening Agent**: Automates initial literature screening.
- **Full-Text Screening Agent**: Screens full-text articles for final inclusion.
- **Data Extraction Agent**: Extracts PICOS data, study characteristics, and outcomes from included studies.
- **Data Analysis Agent**: Performs statistical calculations for network meta-analysis.
- **Certainty of Evidence (CoE) Assessment Agent**: Assists in rating the certainty of evidence using the GRADE approach.

This modular design allows for a flexible and powerful end-to-end solution for evidence synthesis.

## 2. Advanced Features & Capabilities

### 2.1 Parallel Processing Architecture
- **Intelligent Resource Detection**: Automatically detects system CPU, memory, and disk capacity
- **Optimal Worker Allocation**: Recommends optimal number of parallel workers based on system resources
- **Batch Processing**: Distributes documents across multiple workers for maximum efficiency
- **Fault Tolerance**: Individual document failures don't stop the entire assessment process
- **Resource Monitoring**: Dynamic adjustment based on system load and performance

### 2.2 Bilingual Interface Support
- **Interactive Language Selection**: Choose between English and Chinese at startup
- **Comprehensive Translation**: All user messages, progress updates, and error messages in both languages
- **Cultural Adaptation**: Interface design adapted for both Western and Chinese user preferences
- **Runtime Language Switching**: Change language without restarting the application

### 2.3 Advanced Progress Monitoring
- **Real-time Progress Display**: Live updates with percentage completion and ETA
- **Batch Status Tracking**: Monitor individual batch progress and performance metrics
- **Performance Analytics**: Processing rate, average time per document, resource utilization
- **Cost Tracking**: Real-time LLM usage cost monitoring with multi-currency support
- **Interactive Dashboard**: Comprehensive progress visualization with detailed statistics

### 2.4 Checkpoint & Resume System
- **Automatic State Saving**: Periodic checkpoint creation during processing
- **Intelligent Resume**: Continue from last checkpoint after interruptions
- **State Validation**: Automatic detection and recovery from corrupted checkpoints
- **Progress Preservation**: Never lose work due to system failures or interruptions

### 2.5 ROBUST-RCT Assessment Workflow
- **Two-Step Decomposition**: Systematic breakdown of bias assessment into domain-specific evaluations followed by synthesis integration
- **Multi-LLM Validation**: Dual LLM approach with cross-validation for enhanced reliability and confidence scoring
- **Domain-Specific Prompts**: Optimized prompts tailored for each ROBUST-RCT bias domain (randomization, allocation concealment, blinding, incomplete outcome data, selective reporting, other bias)
- **Confidence-Based Classification**: Automatic categorization into high (97.5% accuracy), medium, and low confidence levels based on LLM agreement and evidence clarity
- **Quality Assurance Integration**: Built-in identification of assessments requiring manual review versus those suitable for automated processing
- **Comprehensive Reporting**: Detailed Excel and HTML reports with confidence-coded visual highlighting and domain-specific reasoning

### 2.6 Cost Management & Optimization
- **Multi-Model Cost Tracking**: Track costs separately for different LLM models
- **Currency Support**: Cost reporting in USD, EUR, GBP, CNY, and JPY
- **Usage Analytics**: Detailed token consumption analysis and optimization suggestions
- **Budget Alerts**: Configurable cost threshold warnings and alerts
- **Cost Optimization**: Intelligent recommendations for reducing API costs

## 3. Installation & Setup

### 3.1 System Requirements

**Minimum Requirements:**
- Python 3.8+ (Python 3.10+ recommended)
- 4GB RAM (8GB+ recommended for parallel processing)
- 2GB free disk space
- Internet connection for LLM API access

**Recommended for Optimal Performance:**
- Python 3.10+
- 16GB+ RAM for large document sets
- Multi-core CPU (4+ cores recommended)
- SSD storage for faster I/O operations

**Supported Operating Systems:**
- Windows 10/11
- macOS 10.15+
- Linux (Ubuntu 18.04+, CentOS 7+)

### 3.2 Installation Steps

1. **Clone the Repository**
   ```bash
   git clone https://github.com/enenlhh/SmartNMA-AI-assisted-tools.git
   cd SmartNMA-AI-assisted-tools/robust_rob_assessment_tool
   ```

2. **Create Virtual Environment** (Recommended)
   ```bash
   # Using venv
   python -m venv rob_env
   
   # Activate on Windows
   rob_env\Scripts\activate
   
   # Activate on macOS/Linux
   source rob_env/bin/activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify Installation**
   ```bash
   python main.py --version
   ```

### 3.3 Quick Start Guide

1. **Interactive Mode** (Recommended for beginners)
   ```bash
   python main.py
   ```
   - Select your preferred language (English/Chinese)
   - Follow the interactive menu to configure and start assessment

2. **Command Line Mode** (For advanced users)
   ```bash
   # Start new assessment
   python main.py start -c config/config.json
   
   # Resume interrupted assessment
   python main.py resume -s checkpoint_file.json
   
   # Monitor running assessment
   python main.py monitor
   ```

## 4. ROBUST-RCT Framework and Assessment Workflow

### 4.1 ROBUST-RCT Integration Overview

The SmartEBM Risk of Bias Assessment Tool implements the ROBUST-RCT framework through a sophisticated two-step decomposition process that ensures systematic and reliable bias evaluation:

#### **Step 1: Domain-Specific Assessment**
Each bias domain is evaluated independently using specialized prompts and criteria:

1. **Random Sequence Generation**: Assessment of randomization method adequacy
2. **Allocation Concealment**: Evaluation of allocation sequence concealment
3. **Blinding of Participants and Personnel**: Assessment of performance bias risk
4. **Blinding of Outcome Assessment**: Evaluation of detection bias risk
5. **Incomplete Outcome Data**: Assessment of attrition bias handling
6. **Selective Reporting**: Evaluation of reporting bias risk
7. **Other Bias**: Assessment of additional bias sources

#### **Step 2: Synthesis and Confidence Scoring**
Individual domain assessments are synthesized with confidence classification:

- **High Confidence (97.5% accuracy)**: Clear evidence, consistent LLM agreement, definitive bias determination
- **Medium Confidence**: Minor discrepancies between LLMs, generally clear evidence with minor uncertainties
- **Low Confidence**: Major conflicts between LLMs, insufficient information, or ambiguous evidence requiring manual review

### 4.2 Assessment Workflow Process

#### **Pre-Assessment Phase**
1. **Document Processing**: PDF text extraction with OCR fallback for scanned documents
2. **Content Validation**: Verification of study type and methodological information availability
3. **Batch Organization**: Intelligent grouping for parallel processing optimization

#### **Assessment Execution**
1. **Domain Analysis**: Sequential evaluation of each ROBUST-RCT domain with specialized prompts
2. **Cross-Validation**: Multiple LLM assessment with agreement analysis
3. **Confidence Scoring**: Automatic classification based on evidence clarity and LLM consensus
4. **Quality Flagging**: Identification of assessments requiring manual review

#### **Post-Assessment Processing**
1. **Result Synthesis**: Integration of domain-specific assessments into overall bias evaluation
2. **Report Generation**: Creation of detailed Excel and HTML reports with confidence indicators
3. **Quality Assurance**: Flagging of low-confidence assessments for manual review

### 4.3 Bias Assessment Criteria Configuration

The tool allows customization of bias assessment criteria while maintaining ROBUST-RCT framework compliance:

```json
{
  "rob_framework": {
    "type": "robust_rct",
    "domains": {
      "random_sequence_generation": {
        "weight": 1.0,
        "confidence_threshold": 0.8,
        "require_explicit_method": true
      },
      "allocation_concealment": {
        "weight": 1.0,
        "confidence_threshold": 0.8,
        "require_concealment_method": true
      },
      "blinding_participants": {
        "weight": 0.9,
        "confidence_threshold": 0.7,
        "allow_open_label_justification": true
      }
    }
  }
}
```

## 5. Configuration Guide

### 5.1 Configuration File Setup

The tool uses a comprehensive JSON configuration file. Create your configuration by copying the template:

```bash
cp config/config_template.json config/config.json
```

### 4.2 Essential Configuration Sections

#### **File Paths Configuration**
```json
{
  "paths": {
    "input_folder": "input/documents",
    "output_folder": "output/results", 
    "checkpoint_file": "checkpoints/assessment.json",
    "temp_folder": "temp_parallel",
    "llm_pricing_config": "config/llm_pricing.json"
  }
}
```

#### **Parallel Processing Configuration**
```json
{
  "parallel": {
    "enabled": true,
    "max_workers": 4,
    "max_documents_per_batch": 50,
    "checkpoint_interval": 10,
    "retry_attempts": 3,
    "timeout_seconds": 300,
    "memory_limit_gb": 8.0,
    "auto_detect_workers": true
  }
}
```

#### **LLM Models Configuration**
```json
{
  "llm_models": [
    {
      "name": "Primary Model",
      "api_key": "your_openai_api_key",
      "base_url": "https://api.openai.com/v1",
      "model_name": "gpt-4",
      "max_retries": 3,
      "timeout": 60
    },
    {
      "name": "Secondary Model", 
      "api_key": "your_anthropic_api_key",
      "base_url": "https://api.anthropic.com/v1",
      "model_name": "claude-3-sonnet",
      "max_retries": 3,
      "timeout": 60
    }
  ]
}
```

#### **Cost Tracking Configuration**
```json
{
  "cost_tracking": {
    "enabled": true,
    "currency": "USD",
    "track_by_model": true,
    "generate_reports": true,
    "cost_alerts": true,
    "max_cost_threshold": 100.0
  }
}
```

### 4.3 Configuration Validation

The tool automatically validates your configuration and provides helpful error messages:

```bash
# Validate configuration
python main.py start -c config/config.json --validate-only
```

**Common Configuration Issues:**
- Missing API keys
- Invalid file paths
- Incorrect model names
- Resource limits exceeding system capacity

## 5. Usage Guide

### 5.1 Interactive Mode (Recommended)

Start the tool in interactive mode for guided operation:

```bash
python main.py
```

**Interactive Features:**
1. **Language Selection**: Choose English or Chinese interface
2. **Operation Menu**: Select from available operations with descriptions
3. **System Information**: View system resources and recommendations
4. **Configuration Guidance**: Step-by-step configuration assistance
5. **Progress Monitoring**: Real-time progress updates and statistics

### 5.2 Command Line Operations

#### **Start New Assessment**
```bash
# Basic start with configuration file
python main.py start -c config/config.json

# Start with custom parameters
python main.py start -c config/config.json -w 8 --batch-size 25

# Start with specific input/output directories
python main.py start -c config/config.json -i input/docs -o output/results
```

#### **Resume Interrupted Assessment**
```bash
# Resume from checkpoint
python main.py resume -s checkpoint_file.json

# Resume with different worker count
python main.py resume -s checkpoint_file.json -w 4
```

#### **Monitor Progress**
```bash
# Monitor with auto-detection
python main.py monitor

# Monitor specific assessment
python main.py monitor -s state_file.json -r 10
```

#### **Cleanup Operations**
```bash
# Clean temporary files
python main.py cleanup

# Clean with confirmation
python main.py cleanup --force

# Keep results, clean only temp files
python main.py cleanup --keep-results
```

#### **Merge Results**
```bash
# Merge batch results
python main.py merge -i results/batches -o final_results.xlsx

# Merge with specific format
python main.py merge -i results/batches -o results.json --format json
```

### 5.3 Assessment Workflow

1. **Preparation Phase**
   - System resource detection
   - Configuration validation
   - Input document discovery
   - Batch creation and distribution

2. **Processing Phase**
   - Parallel document processing
   - ROB domain assessment
   - Real-time progress monitoring
   - Automatic checkpoint creation

3. **Completion Phase**
   - Result consolidation
   - Report generation (Excel/HTML)
   - Cost analysis and reporting
   - Cleanup of temporary files

## 6. Output & Results

### 6.1 Generated Files

**Excel Reports (`rob_results.xlsx`)**
- Comprehensive ROB assessment results
- Color-coded confidence levels
- Detailed reasoning for each domain
- Summary statistics and metrics
- Cost breakdown by model

**HTML Visualization (`rob_visualization.html`)**
- Interactive traffic light plots
- Filterable results by confidence level
- Exportable to PDF format
- Batch comparison views

**Cost Reports (`cost_analysis.xlsx`)**
- Detailed token usage statistics
- Multi-currency cost calculations
- Model-specific cost breakdown
- Optimization recommendations

**Checkpoint Files (`*.checkpoint.json`)**
- Assessment state for resumption
- Progress tracking information
- Configuration snapshots
- Error logs and recovery data

### 6.2 Result Interpretation

**Confidence Levels:**
- ðŸŸ¢ **High Confidence**: Both LLMs agree on assessment
- ðŸŸ¡ **Medium Confidence**: Minor discrepancies between LLMs
- ðŸ”´ **Low Confidence**: Major conflicts requiring manual review
- âšª **Uncertain**: Insufficient information for assessment

**Quality Assurance Recommendations:**
1. **Mandatory Review**: All low-confidence and uncertain assessments
2. **Spot Checking**: Random sample of high-confidence results (10-20%)
3. **Domain-Specific Review**: Focus on domains with frequent conflicts
4. **Cost Monitoring**: Regular review of API usage and costs

## 7. Best Practices & Recommendations

### 7.1 Pre-Assessment Validation

**Configuration Testing:**
```bash
# Test configuration with small sample
python main.py start -c config/config.json --start-index 0 --batch-size 5
```

**Prompt Validation:**
1. Review generated prompts in configuration
2. Test on 5-10 representative documents
3. Manually verify assessment accuracy
4. Adjust configuration if needed

### 7.2 Performance Optimization

**System Resource Management:**
- Use auto-detection for optimal worker count
- Monitor memory usage during processing
- Adjust batch size based on document complexity
- Enable checkpoint intervals for long assessments

**Cost Optimization:**
- Use cost tracking and alerts
- Consider model selection based on accuracy vs. cost
- Implement text length limits for large documents
- Review pricing configuration regularly

### 7.3 Quality Control

**Assessment Validation:**
- Always review low-confidence results
- Implement inter-rater reliability checks
- Document assessment criteria and decisions
- Maintain audit trail of manual overrides

**Error Handling:**
- Monitor error logs for systematic issues
- Implement retry logic for transient failures
- Validate document formats before processing
- Backup results and checkpoints regularly

## 8. Troubleshooting

### 8.1 Common Issues

**Installation Problems:**
```bash
# Python version issues
python --version  # Should be 3.8+

# Dependency conflicts
pip install --upgrade pip
pip install -r requirements.txt --force-reinstall

# Permission errors (Linux/macOS)
sudo chown -R $USER:$USER /path/to/tool
```

**Configuration Errors:**
- **Invalid API Keys**: Verify keys are active and have sufficient quota
- **Path Issues**: Use absolute paths or ensure relative paths are correct
- **Model Availability**: Check if specified models are accessible
- **Resource Limits**: Reduce worker count if system resources are insufficient

**Runtime Issues:**
```bash
# Memory errors
# Reduce parallel workers or batch size in config

# Network timeouts
# Increase timeout values in LLM configuration

# Disk space issues
# Clean temporary files: python main.py cleanup
```

### 8.2 Performance Issues

**Slow Processing:**
- Increase parallel workers (within system limits)
- Reduce document text length limits
- Use faster LLM models
- Optimize batch sizes

**High Costs:**
- Enable cost tracking and alerts
- Use smaller/cheaper models for initial assessment
- Implement text preprocessing to reduce token usage
- Review and optimize prompts

**Memory Issues:**
- Reduce parallel workers
- Decrease batch sizes
- Enable memory limits in configuration
- Process documents in smaller chunks

### 8.3 Bias Assessment Specific Issues

**Low Confidence Assessments:**
- **Insufficient Study Information**: Ensure PDFs contain complete methodology sections
- **Ambiguous Reporting**: Consider manual review for studies with unclear methodological descriptions
- **LLM Disagreement**: Review domain-specific assessments to identify sources of conflict
- **Language Barriers**: Verify text extraction quality for non-English studies

**Domain-Specific Challenges:**
```bash
# Randomization Assessment Issues
# Check for explicit randomization method descriptions
# Verify sequence generation details are present

# Allocation Concealment Problems  
# Look for concealment method descriptions
# Ensure allocation timing information is available

# Blinding Assessment Difficulties
# Verify participant and outcome assessor blinding descriptions
# Check for open-label study justifications
```

**Quality Assurance Recommendations:**
- **Mandatory Review**: All low-confidence assessments (typically 10-15% of studies)
- **Spot Checking**: Random sample of high-confidence results (recommended 10-20%)
- **Domain Focus**: Extra attention to domains with frequent conflicts (often blinding and allocation concealment)
- **Inter-rater Reliability**: Periodic validation against manual assessments

**Assessment Accuracy Optimization:**
```json
{
  "quality_control": {
    "confidence_thresholds": {
      "high_confidence_minimum": 0.85,
      "manual_review_maximum": 0.60
    },
    "domain_weights": {
      "randomization": 1.0,
      "allocation_concealment": 1.0,
      "blinding": 0.8
    }
  }
}
```

### 8.4 Getting Help

**Documentation:**
- Check `docs/` directory for detailed guides
- Review configuration template comments
- Consult troubleshooting section
- ROBUST-RCT framework documentation

**Support Channels:**
- GitHub Issues: Report bugs and feature requests
- Email Support: contact@smartebm.org
- Community Forum: [SmartEBM Community](https://community.smartebm.org)
- ROBUST-RCT Framework Support: [Cochrane Methods](https://methods.cochrane.org)

**Debug Information:**
```bash
# Enable verbose logging with bias assessment details
python main.py start -c config.json --verbose --log-file debug.log --include-prompts

# Validate bias assessment configuration
python main.py start -c config.json --validate-rob-config

# Test assessment on sample documents
python main.py start -c config.json --test-mode --sample-size 5
```

## 9. Project Structure

```
robust_rob_assessment_tool/
â”œâ”€â”€ main.py                     # Main entry point with CLI interface
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config_template.json   # Configuration template
â”‚   â”œâ”€â”€ config.json            # User configuration (create from template)
â”‚   â””â”€â”€ llm_pricing.json       # LLM pricing reference
â”œâ”€â”€ core/                      # Core system modules
â”‚   â”œâ”€â”€ parallel_controller.py # Parallel processing management
â”‚   â”œâ”€â”€ progress_monitor.py    # Real-time progress monitoring
â”‚   â”œâ”€â”€ result_merger.py       # Result consolidation
â”‚   â”œâ”€â”€ system_detector.py     # System resource detection
â”‚   â”œâ”€â”€ state_manager.py       # Checkpoint and state management
â”‚   â””â”€â”€ resume_manager.py      # Assessment resumption logic
â”œâ”€â”€ src/                       # Processing engine
â”‚   â”œâ”€â”€ rob_evaluator.py       # ROB assessment logic
â”‚   â”œâ”€â”€ document_processor.py  # Document text extraction
â”‚   â”œâ”€â”€ cost_analyzer.py       # Cost tracking and analysis
â”‚   â”œâ”€â”€ visualizer.py          # HTML report generation
â”‚   â”œâ”€â”€ config_manager.py      # Configuration management
â”‚   â””â”€â”€ data_models.py         # Data structures
â”œâ”€â”€ i18n/                      # Internationalization
â”‚   â”œâ”€â”€ i18n_manager.py        # Language management
â”‚   â””â”€â”€ i18n_config.json       # Bilingual messages
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ README.md              # English documentation
â”‚   â”œâ”€â”€ README_zh.md           # Chinese documentation
â”‚   â””â”€â”€ checkpoint_resume.md   # Checkpoint system guide
â”œâ”€â”€ examples/                  # Usage examples
â”‚   â””â”€â”€ checkpoint_resume_example.py
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â””â”€â”€ resume_cli.py          # Resume command-line interface
â”œâ”€â”€ input/                     # Input documents directory
â”œâ”€â”€ output/                    # Results and reports
â””â”€â”€ temp_parallel/             # Temporary processing files
```

## 10. Advanced Features

### 10.1 Checkpoint System
- **Automatic Checkpointing**: Saves progress every N documents (configurable)
- **Intelligent Resume**: Detects completed work and continues from interruption point
- **State Validation**: Verifies checkpoint integrity and handles corruption
- **Multiple Checkpoints**: Maintains rolling checkpoints for maximum safety

### 10.2 Cost Management
- **Real-time Tracking**: Monitor costs as assessment progresses
- **Multi-currency Support**: View costs in USD, EUR, GBP, CNY, JPY
- **Budget Alerts**: Configurable cost thresholds and warnings
- **Optimization Suggestions**: AI-powered recommendations for cost reduction

### 10.3 Quality Assurance
- **Confidence Scoring**: Automatic assessment of result reliability
- **Conflict Detection**: Identifies disagreements between LLM models
- **Error Isolation**: Individual failures don't affect batch processing
- **Audit Trail**: Complete logging of all assessment decisions

## 11. API Reference

### 11.1 Command Line Interface

```bash
# Main operations
python main.py [OPERATION] [OPTIONS]

# Available operations:
start     # Start new assessment
resume    # Resume from checkpoint  
monitor   # Monitor progress
cleanup   # Clean temporary files
merge     # Merge batch results

# Global options:
-l, --language {en,zh}    # Interface language
-v, --verbose             # Verbose output
-q, --quiet              # Suppress output
--log-file FILE          # Log to file
```

### 11.2 Configuration Schema

**Required Fields:**
- `paths.input_folder`: Input document directory
- `paths.output_folder`: Output results directory
- `llm_models`: At least one LLM configuration

**Optional Fields:**
- `parallel.enabled`: Enable parallel processing (default: true)
- `cost_tracking.enabled`: Enable cost tracking (default: true)
- `rob_framework.type`: ROB framework type (default: "rob2")

## 12. License & Citation

### 12.1 License
This project is licensed under the MIT License. See `LICENSE` file for details.

### 12.2 Citation
If you use this tool in your research, please cite:

```bibtex
@software{smartebm_rob_tool_2024,
  title={SmartEBM ROB Assessment Tool v2.0},
  author={Lai, Haoran and Liu, Jing and Ma, Ning and SmartEBM Group at Lanzhou University},
  year={2024},
  url={https://github.com/enenlhh/SmartNMA-AI-assisted-tools},
  version={2.0}
}
```

**Related Publications:**
> Lai H, Liu J, Ma N, et al. Development and Validation of SmartNMA: An Intelligent Agent for Interventional Systematic Reviews and Network Meta-Analysis.

### 12.3 Acknowledgments
- SmartEBM Group at Lanzhou University at Lanzhou University
- Contributors and beta testers
- Open source community for dependencies and tools

---

**ðŸŽ‰ Start your efficient ROB assessment journey!**

For quick start, go back to the [main README](../README.md) or check out our [Quick Start Guide](quick_start.md).