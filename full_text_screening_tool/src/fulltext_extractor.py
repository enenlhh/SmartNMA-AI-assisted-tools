"""
Full Text Extractor Module
Handles full-text screening workflow without prefiltering
"""

import openai
import os
import csv
import glob
import time
import re
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
from openpyxl import Workbook, load_workbook
from openpyxl.styles import PatternFill, Alignment, Font

from .document_reader import DocumentReader

class FullTextExtractor:
    def __init__(self, screening_llm_configs, prompt_llm_config=None, positive_prompt_path=None, negative_prompt_path=None, config=None):
        """Initialize extractor with LLM configurations and system config"""
        self.screening_llm_configs = screening_llm_configs
        self.prompt_llm_config = prompt_llm_config or list(screening_llm_configs.values())[0]
        # Get first valid LLM name for prompt generation (skip comment fields)
        valid_llm_names = [name for name in screening_llm_configs.keys() if not name.startswith('_')]
        first_valid_name = valid_llm_names[0] if valid_llm_names else list(screening_llm_configs.keys())[0]
        self.prompt_llm_name = "Prompt LLM" if prompt_llm_config else first_valid_name
        
        self.positive_prompt_path = positive_prompt_path
        self.negative_prompt_path = negative_prompt_path
        
        # è¯»å–é…ç½®å‚æ•°
        self.config = config or {}
        self.mode = self.config.get('mode', {})
        self.parallel_settings = self.config.get('parallel_settings', {})
        self.resource_management = self.config.get('resource_management', {})
        
        # å¹¶è¡Œå¤„ç†é…ç½®
        self.screening_mode = self.mode.get('screening_mode', 'single')
        self.parallel_screeners = self.parallel_settings.get('parallel_screeners', 1)
        self.delay_between_screeners = self.resource_management.get('delay_between_screeners', 2)
        
        self.llm_clients = {}
        self.combined_prompt = None
        self.positive_prompt = None
        self.negative_prompt = None
        
        self.tokens_log = []
        self.tokens_csv_path = None
        
        # Initialize screening LLM clients
        for llm_name, config in self.screening_llm_configs.items():
            # Skip comment fields
            if llm_name.startswith('_'):
                continue
            client = openai.OpenAI(
                api_key=config['api_key'],
                base_url=config.get('base_url', 'https://vip.apiyi.com/v1')
            )
            self.llm_clients[llm_name] = client
        
        # Initialize prompt LLM client if separate
        if prompt_llm_config and self.prompt_llm_name not in self.llm_clients:
            prompt_client = openai.OpenAI(
                api_key=prompt_llm_config['api_key'],
                base_url=prompt_llm_config.get('base_url', 'https://vip.apiyi.com/v1')
            )
            self.llm_clients[self.prompt_llm_name] = prompt_client
    
    def create_combined_prompt(self, inclusion_criteria, exclusion_criteria=None, prompt_file_path=None):
        """Create combined prompt for full-text screening"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print(f"\nğŸ¯ {get_message('prompt_generation')}")
        
        # åŸºç¡€promptæ¨¡æ¿
        base_prompt = """You are a systematic review expert assisting with full text screening of research papers.
        Your task has two parts:
        
        PART 1: Extract the following PICOS information from the full text of a research paper:
        1. Study design - Be specific about the exact type of study. Only mark as randomized controlled trial if explicitly stated. Always extract the MOST SPECIFIC design mentioned.
        2. Participants - Include details of study subjects, demographics, conditions, severity, setting, etc.
        3. Intervention - Full description of the intervention being studied, including dosage, frequency, duration when available.
        4. Comparison - What the intervention is compared against, be specific about control groups.
        5. Outcomes - Primary and secondary outcomes measured in the study, including timepoints if mentioned.
        
        If any information is not explicitly mentioned, respond with "Not reported" for that category.
        If it is not possible to extract relevant information because a study is clearly irrelevant, respond with "Not available".
        """

        # æ£€æŸ¥å¹¶åŠ è½½ç°æœ‰æç¤ºè¯æ–‡ä»¶æˆ–ç”Ÿæˆæ–°æç¤ºè¯
        if prompt_file_path and os.path.exists(prompt_file_path):
            print(f"âœ“ Found existing prompt file: {os.path.basename(prompt_file_path) if prompt_file_path else 'Unknown'}")
            print("Using existing screening guidance without regenerating")
            with open(prompt_file_path, "r", encoding='utf-8') as f:
                screening_guidance = f.read()
        else:
            print("Generating new full-text screening guidance...")
            screening_guidance = self._generate_fulltext_screening_guidance(inclusion_criteria, exclusion_criteria, prompt_file_path)
            if screening_guidance is None:
                print("âš ï¸ Falling back to basic screening format")
                return self._create_fallback_fulltext_prompt(base_prompt, inclusion_criteria, exclusion_criteria)

        # Task 2 prompt
        task2_prompt = f"""PART 2: Based on the extracted PICOS information, determine if the study meets ALL of the inclusion criteria:
        {screening_guidance}
        General Full-Text Screening Principles:
        - INCLUDE: If the full text clearly fulfills all inclusion criteria and no exclusion criteria
        - EXCLUDE: If the full text clearly violates any inclusion criterion or meets any exclusion criterion
        - EXCLUDE: If the full text is clearly not relevant to the inclusion criteria
        - UNCLEAR: Only if information is truly insufficient (rare in full-text, as complete information is available)
        - Be more definitive than title/abstract screening; use all available details for decision
        - Focus on conceptual matching rather than exact terminology
        - **Before finishing, MUST re-check all PICOS criteria systematically**
        Format your response EXACTLY as follows:
        
        ===PICOS EXTRACTION===
        Study design: [detailed extracted study design information]
        Participants: [detailed extracted participants information]
        Intervention: [detailed extracted intervention information]
        Comparison: [detailed extracted comparison information]
        Outcomes: [detailed extracted outcomes information]
        ===INCLUSION ASSESSMENT===
        Decision: [INCLUDE/EXCLUDE/UNCLEAR - choose only one]
        Explanation: [brief rationale in ONE SENTENCE, 50 WORDS MAXIMUM, focus on PRIMARY reason]
        """
        
        complete_prompt = base_prompt + task2_prompt
        final_template_tokens = len(complete_prompt) // 4
        print(f"âœ“ Prompt template ready (~{final_template_tokens:,} tokens)")
        return complete_prompt

    def _generate_fulltext_screening_guidance(self, inclusion_criteria, exclusion_criteria, prompt_file_path):
        """Generate new full-text screening guidance"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print("ğŸ“‹ Processing criteria and generating detailed screening guidance...")
        
        # å‡†å¤‡æ ‡å‡†æ–‡æœ¬
        print("\nğŸ” Analyzing inclusion/exclusion criteria:")
        inclusion_details = {}
        exclusion_details = {}
        
        for key, value in inclusion_criteria.items():
            if value and value not in ["No limit", "None"]:
                inclusion_details[key] = value
                print(f"  âœ“ Include - {key}: {value[:80]}{'...' if len(value) > 80 else ''}")
        
        if exclusion_criteria:
            for key, value in exclusion_criteria.items():
                if value and value not in ["No limit", "None"]:
                    exclusion_details[key] = value
                    print(f"  âœ— Exclude - {key}: {value[:80]}{'...' if len(value) > 80 else ''}")

        # ä¸ºå…¨æ–‡ç­›é€‰å®šåˆ¶çš„promptç”ŸæˆæŒ‡ä»¤
        screening_prompt = f"""As a systematic review expert, create COMPREHENSIVE full-text screening guidance based on these criteria:

        INCLUSION CRITERIA:
        {chr(10).join(f"{k}: {v}" for k, v in inclusion_details.items())}
        
        {"EXCLUSION CRITERIA:" if exclusion_details else ""}
        {chr(10).join(f"{k}: {v}" for k, v in exclusion_details.items()) if exclusion_details else ""}
        
        Create guidance that:
        1. Provides direct instructions for each PICOS criterion
        2. Emphasizes definitive decisions using full text's complete information
        3. Converts any indirect/ambiguous criteria into explicit, actionable statements
        4. Keeps it brief and professional, trusting screener's knowledge
        
        FORMAT YOUR GUIDANCE LIKE THIS:
        
        STUDY DESIGN SCREENING:
        Must Include: [Detailed description of acceptable study designs based on criteria]
        Must Exclude: [Detailed description of study designs that should be excluded]
        Decision Rules: [Specific rules for making inclusion decisions based on study design]
        
        PARTICIPANTS SCREENING:
        Must Include: [Detailed description of target population characteristics]
        Must Exclude: [Detailed description of populations that should be excluded]
        Decision Rules: [Specific rules for evaluating participant eligibility]
        
        INTERVENTION SCREENING:
        Must Include: [Detailed description of target interventions]
        Must Exclude: [Detailed description of interventions that should be excluded]
        Decision Rules: [Specific rules for evaluating intervention relevance]
        
        COMPARISON SCREENING:
        Must Include: [Detailed description of acceptable comparisons]
        Must Exclude: [Detailed description of comparisons that should be excluded]
        Decision Rules: [Specific rules for evaluating comparison groups]
        
        OUTCOMES SCREENING:
        Must Include: [Detailed description of target outcomes]
        Must Exclude: [Detailed description of outcomes that should be excluded]
        Decision Rules: [Specific rules for evaluating outcome relevance]
        
        OVERALL DECISION FRAMEWORK:
        - INCLUDE: Study meets ALL inclusion criteria and NO exclusion criteria
        - EXCLUDE: Study fails to meet ANY inclusion criterion OR meets ANY exclusion criterion
        - UNCLEAR: Insufficient information to make definitive decision (rare in full-text screening)
        
        IMPORTANT NOTES:
        - Full-text screening allows for more definitive decisions than title/abstract screening
        - Use the complete study information available in the full text
        - Be thorough but decisive - avoid UNCLEAR unless truly ambiguous
        - Focus on conceptual matching rather than exact terminology
        - Consider the study's primary focus and main findings
        
        Your guidance will be used for comprehensive full-text evaluation of research papers.
        """

        # ä¼°ç®—prompt tokens
        prompt_tokens_estimate = len(screening_prompt) // 4
        print(f"ğŸ“Š Estimated tokens for guidance generation: ~{prompt_tokens_estimate:,}")
        
        max_retries = 5
        for retry_count in range(max_retries):
            try:
                print(f"ğŸ¤– Sending request to {self.prompt_llm_name} for guidance generation...")
                client = self.llm_clients[self.prompt_llm_name]
                config = self.prompt_llm_config if self.prompt_llm_name == "Prompt LLM" else self.screening_llm_configs[self.prompt_llm_name]
                
                response = client.chat.completions.create(
                    model=config['model'],
                    messages=[
                        {"role": "system", "content": "You are an evidence-based medicine expert specializing in systematic reviews and full-text screening."},
                        {"role": "user", "content": screening_prompt}
                    ],
                    temperature=0.1,
                    max_tokens=4000  # å¢åŠ tokené™åˆ¶ä»¥é€‚åº”æ›´è¯¦ç»†çš„å…¨æ–‡æŒ‡å¯¼
                )
                
                screening_guidance = response.choices[0].message.content
                
                try:
                    prompt_tokens = response.usage.prompt_tokens
                    completion_tokens = response.usage.completion_tokens
                    total_tokens = response.usage.total_tokens
                    print(f"âœ“ Tokens used: {total_tokens:,} (Prompt: {prompt_tokens:,}, Completion: {completion_tokens:,})")
                except AttributeError:
                    guidance_tokens_estimate = len(screening_guidance) // 4
                    print(f"âœ“ Estimated tokens for generated guidance: ~{guidance_tokens_estimate:,}")
                
                print("âœ… Full-text screening guidance successfully generated!")
                if prompt_file_path:
                    with open(prompt_file_path, "w", encoding='utf-8') as f:
                        f.write(screening_guidance)
                    
                    print(f"\n" + "="*60)
                    print("ğŸ‰ FULL-TEXT SCREENING GUIDANCE GENERATED")
                    print("="*60)
                    print("The full-text screening guidance has been successfully generated!")
                    print(f"\nğŸ“ Generated File:")
                    print(f"   â€¢ Full-Text Screening Guidance: {prompt_file_path}")
                    
                    print("\nğŸ“‹ Next Steps:")
                    print("1. Please review and adjust the generated full-text screening guidance as needed")
                    print("2. The guidance contains detailed instructions for comprehensive full-text evaluation")
                    print("3. After reviewing, restart the program to begin full-text screening process")
                    
                    print(f"\nğŸ”„ To restart: python run_fulltext.py")
                    print("="*60)
                    
                    import sys
                    sys.exit(0)
                else:
                    return screening_guidance
                
            except Exception as e:
                if retry_count < max_retries - 1:
                    print(f"Error generating full-text guidance, attempt {retry_count + 1}/{max_retries}: {str(e)}. Retrying...")
                    time.sleep(2 ** retry_count)
                else:
                    print(f"\nERROR: Failed to generate full-text screening guidance after {max_retries} attempts: {str(e)}")
                    return None

    def _create_fallback_fulltext_prompt(self, base_prompt, inclusion_criteria, exclusion_criteria):
        """Create fallback prompt for full-text screening"""
        print("Falling back to basic full-text screening format...")
        
        inclusion_details = {k: v for k, v in inclusion_criteria.items() if v and v not in ["No limit", "None"]}
        exclusion_details = {k: v for k, v in exclusion_criteria.items() if v and v not in ["No limit", "None"]} if exclusion_criteria else {}
        
        # ç®€å•çš„fallbackæ–¹æ¡ˆï¼Œä¸“é—¨é’ˆå¯¹å…¨æ–‡ç­›é€‰
        fallback_guidance = "### Full-Text Screening Guidance\n\n"
        fallback_guidance += "**Inclusion Criteria (All must be met):**\n"
        for key, value in inclusion_details.items():
            fallback_guidance += f"- {key}: {value}\n  * Evaluate based on complete study information in full text\n  * Look for explicit statements and detailed descriptions\n\n"
        
        if exclusion_details:
            fallback_guidance += "\n**Exclusion Criteria (Any one excludes the study):**\n"
            for key, value in exclusion_details.items():
                fallback_guidance += f"- {key}: {value}\n  * Exclude if clearly meeting this criterion based on full text\n\n"
        
        fallback_guidance += "\n**Full-Text Screening Principles:**\n"
        fallback_guidance += "- Use all available information in the complete document\n"
        fallback_guidance += "- Be more definitive than title/abstract screening\n"
        fallback_guidance += "- INCLUDE only if ALL inclusion criteria are met\n"
        fallback_guidance += "- EXCLUDE if ANY exclusion criterion is met or ANY inclusion criterion is not met\n"
        fallback_guidance += "- UNCLEAR should be rare since full text provides complete information\n"
        fallback_guidance += "- Focus on the study's main objectives and primary findings\n"
        
        # æ·»åŠ åˆ°Task 2 prompt
        task2_prompt = f"""PART 2: Based on the extracted PICOS information, determine if the study meets ALL inclusion criteria:

        {fallback_guidance}

        Format your response EXACTLY as follows:
        
        <START_PICOS>
        <STUDY_DESIGN>detailed extracted study design information</STUDY_DESIGN>
        <PARTICIPANTS>detailed extracted participants information</PARTICIPANTS>
        <INTERVENTION>detailed extracted intervention information</INTERVENTION>
        <COMPARISON>detailed extracted comparison information</COMPARISON>
        <OUTCOMES>detailed extracted outcomes information</OUTCOMES>
        </START_PICOS>

        <DECISION>INCLUDE or EXCLUDE or UNCLEAR - choose only one of these three options</DECISION>

        <EXPLANATION>
        detailed explanation for your decision, citing specific criteria that were met or not met
        </EXPLANATION>
        """
        
        complete_prompt = base_prompt + task2_prompt
        fallback_template_tokens = len(complete_prompt) // 4
        print(f"\nEstimated tokens for fallback full-text prompt template: ~{fallback_template_tokens} tokens")
        print("Using fallback full-text screening guidance due to LLM error.")
        return complete_prompt

    
    def load_or_generate_positive_negative_prompts(self, normal_prompt):
        """Load or generate positive and negative versions of prompt for full-text screening"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print(f"\nğŸ” {get_message('positive_negative_check')}")
        
        # Check if files exist
        positive_exists = self.positive_prompt_path and os.path.exists(self.positive_prompt_path)
        negative_exists = self.negative_prompt_path and os.path.exists(self.negative_prompt_path)
        
        if positive_exists and negative_exists:
            print(f"âœ“ Found positive prompt: {os.path.basename(self.positive_prompt_path) if self.positive_prompt_path else 'Unknown'}")
            print(f"âœ“ Found negative prompt: {os.path.basename(self.negative_prompt_path) if self.negative_prompt_path else 'Unknown'}")
            print(get_message("variants_found"))
            
            # Load from files directly
            if self.positive_prompt_path and self.negative_prompt_path:
                with open(self.positive_prompt_path, "r", encoding='utf-8') as f:
                    positive_prompt = f.read()
                with open(self.negative_prompt_path, "r", encoding='utf-8') as f:
                    negative_prompt = f.read()
            else:
                print("âš ï¸ Prompt file paths not properly set")
                return self.generate_positive_negative_fulltext_prompts(normal_prompt)
                
            print("âœ… Successfully loaded existing prompt variants")
            return positive_prompt, negative_prompt
        
        else:
            print(get_message("generating_variants"))
            return self.generate_positive_negative_fulltext_prompts(normal_prompt)
            
    def generate_positive_negative_fulltext_prompts(self, normal_prompt):
        """Generate positive and negative versions of full-text screening prompt"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print("ğŸ”„ Generating positive and negative prompt variants...")
        
        # å…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰æœ‰æ•ˆçš„æç¤ºæ–‡ä»¶
        if (self.positive_prompt_path and os.path.exists(self.positive_prompt_path) and 
            self.negative_prompt_path and os.path.exists(self.negative_prompt_path)):
            
            if self.positive_prompt_path and self.negative_prompt_path:
                with open(self.positive_prompt_path, "r", encoding='utf-8') as f:
                    positive_content = f.read().strip()
                with open(self.negative_prompt_path, "r", encoding='utf-8') as f:
                    negative_content = f.read().strip()
            else:
                # å¦‚æœè·¯å¾„ä¸º Noneï¼Œè·³è¿‡æ–‡ä»¶æ£€æŸ¥ç›´æ¥ç”Ÿæˆæ–°çš„
                positive_content = ""
                negative_content = ""
                
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«å®é™…å†…å®¹ï¼ˆä¸æ˜¯å ä½ç¬¦ï¼‰
            if (positive_content and negative_content and 
                "[restructured full-text prompt" not in positive_content and 
                "[restructured full-text prompt" not in negative_content and
                len(positive_content) > 500 and len(negative_content) > 500):  # å…¨æ–‡promptæ›´é•¿
                print("âœ“ Found valid existing prompt variant files")
                return positive_content, negative_content
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ•ˆï¼Œç”Ÿæˆæ–°çš„å…¨æ–‡ç­›é€‰æç¤º
        prompt_generation_instruction = f"""
        Based on the following original full-text screening prompt, create TWO versions with subtly different presentation styles while preserving ALL information content exactly:
        
        ORIGINAL FULL-TEXT PROMPT:
        {normal_prompt}
        
        Create these two versions specifically optimized for full-text screening:
        
        1. VERSION A (Comprehensive Full-Text Evaluation): 
        - Present criteria with emphasis on thorough, systematic evaluation of complete document
        - Use language that encourages comprehensive assessment of all available information
        - Structure information to promote detailed examination of full study content
        - Emphasize the advantage of having complete study information for decision-making
        
        2. VERSION B (Precision-Focused Full-Text Analysis):
        - Present criteria with emphasis on precise, targeted evaluation of key study elements
        - Use language that emphasizes meeting exact requirements based on full study details
        - Structure information to encourage focused verification of specific criteria
        - Emphasize accuracy and specificity in decision-making with complete information
        
        CRITICAL REQUIREMENTS FOR FULL-TEXT SCREENING:
        - Do NOT add, remove, or modify ANY factual content or criteria
        - Do NOT change any specific requirements or thresholds
        - Do NOT use obvious directional language like "lean toward inclusion/exclusion"
        - ONLY change the presentation flow and subtle linguistic framing
        - Preserve all PICOS extraction instructions exactly
        - Maintain the same response format requirements (===PICOS EXTRACTION=== ... ===INCLUSION ASSESSMENT=== Decision: ... Explanation: ...)
        - Keep all technical details identical
        - Use neutral, professional tone throughout
        - Account for the fact that full-text provides complete study information
        - Maintain the higher decisiveness expected in full-text vs title/abstract screening
        
        The two versions should feel naturally different in their approach to full-text evaluation but not obviously biased toward any particular outcome.
        
        Please format your response as:
        ===VERSION A (COMPREHENSIVE FULL-TEXT)===
        [restructured full-text prompt with comprehensive evaluation approach]
        
        ===VERSION B (PRECISION-FOCUSED FULL-TEXT)===
        [restructured full-text prompt with precision-focused approach]
        """
                
        max_retries = 3
        for retry_count in range(max_retries):
            try:
                client = self.llm_clients[self.prompt_llm_name]
                config = self.prompt_llm_config if self.prompt_llm_name == "Prompt LLM" else self.screening_llm_configs[self.prompt_llm_name]
                
                response = client.chat.completions.create(
                    model=config['model'],
                    messages=[
                        {"role": "system", "content": "You are an expert at restructuring full-text screening content with different psychological framing while preserving all factual information exactly."},
                        {"role": "user", "content": prompt_generation_instruction}
                    ],
                    temperature=0.1,
                    max_tokens=6000  # å¢åŠ tokené™åˆ¶ï¼Œå› ä¸ºå…¨æ–‡promptæ›´é•¿
                )
                
                result = response.choices[0].message.content
                
                # æ”¹è¿›çš„è§£æé€»è¾‘ï¼Œé€‚é…å…¨æ–‡ç­›é€‰
                if "===VERSION A (COMPREHENSIVE FULL-TEXT)===" in result and "===VERSION B (PRECISION-FOCUSED FULL-TEXT)===" in result:
                    try:
                        # åˆ†å‰²å“åº”
                        parts = result.split("===VERSION B (PRECISION-FOCUSED FULL-TEXT)===")
                        if len(parts) >= 2:
                            positive_prompt = parts[0].replace("===VERSION A (COMPREHENSIVE FULL-TEXT)===", "").strip()
                            negative_prompt = parts[1].strip()
                            
                            # éªŒè¯å†…å®¹ä¸ä¸ºç©ºä¸”ä¸æ˜¯å ä½ç¬¦
                            if (positive_prompt and negative_prompt and 
                                len(positive_prompt) > 500 and len(negative_prompt) > 500 and
                                "[restructured full-text prompt" not in positive_prompt and 
                                "[restructured full-text prompt" not in negative_prompt):
                                
                                # ä¿å­˜åˆ°æ–‡ä»¶
                                if self.positive_prompt_path:
                                    with open(self.positive_prompt_path, "w", encoding='utf-8') as f:
                                        f.write(positive_prompt)
                                    print(f"âœ“ Positive full-text prompt (comprehensive evaluation) saved to: {self.positive_prompt_path}")
                                
                                if self.negative_prompt_path:
                                    with open(self.negative_prompt_path, "w", encoding='utf-8') as f:
                                        f.write(negative_prompt)
                                    print(f"âœ“ Negative full-text prompt (precision-focused) saved to: {self.negative_prompt_path}")
                                
                                print("âœ“ Successfully generated and saved Positive and Negative full-text prompt versions")
                                
                                # ç›´æ¥é€€å‡ºç¨‹åºï¼Œæç¤ºç”¨æˆ·æ£€æŸ¥æ–‡ä»¶
                                print(f"\n" + "="*60)
                                print("ğŸ‰ FULL-TEXT PROMPT GENERATION COMPLETED")
                                print("="*60)
                                print("Positive and Negative full-text prompt variants have been successfully generated!")
                                print("\nğŸ“ Generated Files:")
                                if self.positive_prompt_path:
                                    print(f"   â€¢ Positive Full-Text Prompt (Comprehensive): {self.positive_prompt_path}")
                                if self.negative_prompt_path:
                                    print(f"   â€¢ Negative Full-Text Prompt (Precision-Focused): {self.negative_prompt_path}")
                                
                                print("\nğŸ“‹ Next Steps:")
                                print("1. Please review and adjust the generated full-text prompt files as needed")
                                print("2. The prompts use different psychological framing for full-text evaluation")
                                print("3. After reviewing, restart the program to begin full-text screening process")
                                print("4. The system will automatically use the generated full-text prompt variants")
                                
                                print(f"\nğŸ”„ To restart: python run_fulltext.py")
                                print("="*60)
                                
                                # ç›´æ¥é€€å‡ºç¨‹åº
                                import sys
                                sys.exit(0)
                            else:
                                raise ValueError("Generated full-text prompts are too short, contain placeholders, or don't have the expected structure")
                        else:
                            raise ValueError("Could not split full-text response properly")
                    except Exception as parse_error:
                        print(f"Parsing error: {parse_error}")
                        raise ValueError(f"Failed to parse generated full-text prompt versions: {parse_error}")
                else:
                    raise ValueError("Response does not contain expected section markers for full-text prompts")
                    
            except Exception as e:
                if retry_count < max_retries - 1:
                    print(f"Failed to generate full-text prompt variants, retry {retry_count + 1}/{max_retries}: {str(e)}")
                    time.sleep(2)
                else:
                    print(f"Failed to generate full-text prompt variants after {max_retries} attempts: {str(e)}")
                    print("Using original full-text prompt for both positive and negative versions")
                    
                    # ä½¿ç”¨åŸå§‹æç¤ºä½œä¸ºå›é€€
                    if self.positive_prompt_path:
                        with open(self.positive_prompt_path, "w", encoding='utf-8') as f:
                            f.write(normal_prompt)
                        print(f"âœ“ Fallback: Original full-text prompt saved as positive prompt to: {self.positive_prompt_path}")
                    
                    if self.negative_prompt_path:
                        with open(self.negative_prompt_path, "w", encoding='utf-8') as f:
                            f.write(normal_prompt)
                        print(f"âœ“ Fallback: Original full-text prompt saved as negative prompt to: {self.negative_prompt_path}")
                    
                    # å›é€€æƒ…å†µä¹Ÿç›´æ¥é€€å‡º
                    print(f"\n" + "="*60)
                    print("âš ï¸  FULL-TEXT PROMPT GENERATION FALLBACK")
                    print("="*60)
                    print("Due to generation issues, the original full-text prompt has been saved to both files.")
                    print("\nğŸ“ Fallback Files:")
                    if self.positive_prompt_path:
                        print(f"   â€¢ Positive Full-Text Prompt: {self.positive_prompt_path}")
                    if self.negative_prompt_path:
                        print(f"   â€¢ Negative Full-Text Prompt: {self.negative_prompt_path}")
                    
                    print("\nğŸ“‹ Next Steps:")
                    print("1. Please manually edit the full-text prompt files to create different variants")
                    print("2. Adjust the psychological framing for full-text screening as needed")
                    print("3. After editing, restart the program to begin full-text screening process")
                    
                    print(f"\nğŸ”„ To restart: python run_fulltext.py")
                    print("="*60)
                    
                    import sys
                    sys.exit(0)
        
        return normal_prompt, normal_prompt

    def process_study_sequential(self, document_content, inclusion_criteria, study_id):
        """Sequential processing of one full-text study: LLM A processes first, then LLM B uses appropriate prompt based on result"""
        print("  Starting sequential processing of one full-text study...")
        
        # Ensure we have the combined prompt
        if self.combined_prompt is None:
            self.combined_prompt = self.create_combined_prompt(inclusion_criteria)
        
        # Load or generate positive and negative versions of prompt
        if self.positive_prompt is None or self.negative_prompt is None:
            self.positive_prompt, self.negative_prompt = self.load_or_generate_positive_negative_prompts(self.combined_prompt)
        
        # Get LLM names list (filter out comment fields)
        all_llm_names = list(self.screening_llm_configs.keys())
        llm_names = [name for name in all_llm_names if not name.startswith('_')]
        if len(llm_names) < 2:
            raise ValueError(f"At least 2 LLM instances required for sequential processing, found {len(llm_names)}: {llm_names}")
        
        llm_a_name = llm_names[0]  # First LLM
        llm_b_name = llm_names[1]  # Second LLM
        
        results = {}
        
        # Step 1: LLM A processes with normal prompt
        print(f"  Step 1: {llm_a_name} processing full-text with normal prompt...")
        try:
            picos_a, inclusion_a = self.process_fulltext_with_prompt(
                llm_a_name, document_content, self.combined_prompt, study_id
            )
            results[llm_a_name] = {
                'picos': picos_a,
                'inclusion': inclusion_a
            }
            print(f"  {llm_a_name} completed: {inclusion_a}")
            
            # Determine LLM A's decision
            if "INCLUDE" in inclusion_a:
                llm_a_decision = "INCLUDE"
            elif "EXCLUDE" in inclusion_a:
                llm_a_decision = "EXCLUDE"
            else:
                llm_a_decision = "UNCLEAR"
            
            # Step 2: Select prompt for LLM B based on LLM A's result
            if llm_a_decision == "INCLUDE":
                selected_prompt = self.negative_prompt
                prompt_type = "negative prompt (precision-focused evaluation)"
                print(f"  LLM A result is INCLUDE, LLM B will use {prompt_type}")
            elif llm_a_decision == "EXCLUDE":
                selected_prompt = self.positive_prompt  
                prompt_type = "positive prompt (comprehensive evaluation)"
                print(f"  LLM A result is EXCLUDE, LLM B will use {prompt_type}")
            else:
                # Use positive prompt for UNCLEAR cases
                selected_prompt = self.positive_prompt
                prompt_type = "positive prompt (comprehensive evaluation, because LLM A result is unclear)"
                print(f"  LLM A result is UNCLEAR, LLM B will use {prompt_type}")
            
            # Step 3: LLM B processes with selected prompt
            print(f"  Step 2: {llm_b_name} processing full-text with {prompt_type}...")
            picos_b, inclusion_b = self.process_fulltext_with_prompt(
                llm_b_name, document_content, selected_prompt, study_id
            )
            results[llm_b_name] = {
                'picos': picos_b,
                'inclusion': inclusion_b,
                'prompt_type': prompt_type
            }
            print(f"  {llm_b_name} completed: {inclusion_b}")
            
        except Exception as e:
            print(f"  Sequential full-text processing error: {str(e)}")
            # If error occurs, return error information
            error_result = {
                'picos': {
                    "study_design": f"Error: {str(e)}",
                    "participants": f"Error: {str(e)}",
                    "intervention": f"Error: {str(e)}",
                    "comparison": f"Error: {str(e)}",
                    "outcomes": f"Error: {str(e)}"
                },
                'inclusion': f"UNCLEAR (Error: {str(e)})"
            }
            results[llm_a_name] = error_result
            results[llm_b_name] = error_result
        
        print(f"  Sequential full-text processing completed")
        return results

    def extract_study_id(self, document_text, llm_name):
        """Extract study ID (author surname and year)"""
        client = self.llm_clients[llm_name]
        config = self.screening_llm_configs[llm_name]
        
        prompt = f"""
        Extract from the text:
        1. The surname of the first author
        2. The publication year (4 digits), if you can't find the year, use the newest year you can find in the text
        Return format: "Surname, YYYY"
        If not found, return: "Unknown, 0000"
        Text:
        {document_text[:1500]}
        """
        
        max_retries = 3
        for retry_count in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=config['model'],
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.0,
                )
                
                result = response.choices[0].message.content.strip()
                
                # 1. é¦–å…ˆå°è¯•ä½¿ç”¨åŸå§‹æ­£åˆ™è¡¨è¾¾å¼
                if re.match(r"^[A-Za-z\-'\s]+,?\s*\d{4}$", result):
                    return result
                
                # 2. å¦‚æœåŸå§‹æ­£åˆ™è¡¨è¾¾å¼å¤±è´¥ï¼Œå°è¯•ä¸€ä¸ªæ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼
                match = re.search(r"([A-Za-z\-'\s]+)[\s,]+(\d{4})", result)
                if match:
                    surname = match.group(1).strip()
                    year = match.group(2)
                    formatted_result = f"{surname}, {year}"
                    return formatted_result
                
                # 3. å°è¯•æŸ¥æ‰¾ä»»ä½•å§“æ°å’Œå››ä½æ•°å­—çš„ç»„åˆ
                name_match = re.search(r"([A-Za-z\-'\s]+)", result)
                year_match = re.search(r"(\d{4})", result)
                if name_match and year_match:
                    surname = name_match.group(1).strip()
                    year = year_match.group(1)
                    formatted_result = f"{surname}, {year}"
                    return formatted_result
                
                return "Unknown, 0000"
                
            except Exception as e:
                if retry_count < max_retries - 1:
                    print(f"  Error extracting study ID: {str(e)}. Retrying {retry_count+1}/{max_retries}...")
                    time.sleep(2)
                else:
                    print(f"  Failed to extract study ID after {max_retries} attempts: {str(e)}")
                    return "Unknown, 0000"
    
    def process_single_document(self, file_path, inclusion_criteria, file_index, total_files):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            # Fallback if i18n not available
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        file_name = os.path.basename(file_path)
        
        try:
            # è·å–æ–‡ä»¶å¤§å°
            try:
                file_size_kb = os.path.getsize(file_path) / 1024
            except:
                file_size_kb = 0
            
            # è¯»å–æ–‡æ¡£å†…å®¹
            document_content = DocumentReader.read_file(file_path)
            if document_content is None:
                return None
            
            # æ˜¾ç¤ºæå–çš„æ–‡æœ¬ä¿¡æ¯
            char_count = len(document_content)
            
            # æå–ç ”ç©¶ID
            valid_llm_names = [name for name in self.screening_llm_configs.keys() if not name.startswith('_')]
            llm_name = valid_llm_names[0] if valid_llm_names else list(self.screening_llm_configs.keys())[0]
            study_id = self.extract_study_id(document_content, llm_name)
            
            # å¤„ç†æ–‡æ¡£ï¼ˆæ— é¢„ç­›é€‰ï¼‰
            start_time = time.time()
            study_results = self.process_study_sequential(document_content, inclusion_criteria, study_id)
            end_time = time.time()
            
            # å‡†å¤‡ç»“æœæ•°æ®
            results = []
            for llm_name, result in study_results.items():
                result_data = {
                    'filename': file_name,
                    'filepath': file_path,
                    'study_id': study_id,
                    'llm_name': llm_name,
                    'picos': result['picos'],
                    'inclusion': result['inclusion']
                }
                results.append(result_data)
            
            return results
            
        except Exception as e:
            return None
    
    def process_documents_parallel(self, folder_path, inclusion_criteria, output_path, exclusion_criteria=None, prompt_file_path=None):
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡æ¡£"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            # Fallback if i18n not available
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print(f"\n=== Parallel Processing Mode: {self.parallel_screeners} threads ===")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        file_patterns = [
            os.path.join(folder_path, "*.pdf"),
            os.path.join(folder_path, "*.docx"),
            os.path.join(folder_path, "*.doc"),
            os.path.join(folder_path, "*.txt")
        ]
        
        all_files = []
        for pattern in file_patterns:
            all_files.extend(glob.glob(pattern))
        
        if not all_files:
            print(get_message("system_error", error=f"No supported files found in {folder_path}"))
            return
        
        all_files.sort()
        total_files = len(all_files)
        print(get_message("detected_documents", count=total_files))
        
        # åˆ›å»ºç­›é€‰æç¤ºè¯
        if self.combined_prompt is None:
            print(get_message("prompt_generation"))
            self.combined_prompt = self.create_combined_prompt(inclusion_criteria, exclusion_criteria, prompt_file_path)
            print(get_message("prompt_loaded"))
        
        results = []
        start_time = time.time()
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=self.parallel_screeners) as executor:
                print(f"\n{get_message('batch_processing')}")
                print(get_message("batch_started", batch_id="Parallel", count=total_files))
                
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_file = {}
                for i, file_path in enumerate(all_files):
                    # åœ¨ç­›é€‰å™¨ä¹‹é—´æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å… API é™åˆ¶
                    if i > 0 and i % self.parallel_screeners == 0:
                        time.sleep(self.delay_between_screeners)
                    
                    future = executor.submit(self.process_single_document, file_path, inclusion_criteria, i, total_files)
                    future_to_file[future] = file_path
                
                # æ”¶é›†ç»“æœ
                completed = 0
                last_progress_time = time.time()
                
                for future in as_completed(future_to_file):
                    file_path = future_to_file[future]
                    file_name = os.path.basename(file_path)
                    
                    try:
                        file_results = future.result()
                        if file_results:
                            results.extend(file_results)
                            print(get_message("document_completed", filename=file_name))
                        else:
                            print(get_message("document_failed", filename=file_name, error="processing failed"))
                        
                        completed += 1
                        
                        # æ˜¾ç¤ºè¿›åº¦ç»Ÿè®¡ï¼ˆæ¯5ç§’æˆ–æ¯10ä¸ªæ–‡æ¡£æ›´æ–°ä¸€æ¬¡ï¼‰
                        current_time = time.time()
                        if (current_time - last_progress_time > 5) or (completed % 10 == 0) or (completed == total_files):
                            elapsed_time = current_time - start_time
                            speed = completed / (elapsed_time / 60) if elapsed_time > 0 else 0
                            remaining_docs = total_files - completed
                            
                            print(f"\nğŸ“Š {get_message('batch_progress', batch_id='Parallel', processed=completed, total=total_files)}")
                            if speed > 0:
                                print(get_message("processing_speed", speed=f"{speed:.1f}"))
                                if remaining_docs > 0:
                                    estimated_remaining = (remaining_docs / speed) * 60  # seconds
                                    remaining_minutes = int(estimated_remaining / 60)
                                    remaining_seconds = int(estimated_remaining % 60)
                                    print(get_message("estimated_time", time=f"{remaining_minutes}m {remaining_seconds}s"))
                            
                            last_progress_time = current_time
                        
                    except Exception as e:
                        print(get_message("document_failed", filename=file_name, error=str(e)))
                        completed += 1
                
                print(f"\n{get_message('all_batches_completed')}")
        
        except KeyboardInterrupt:
            print(f"\n{get_message('task_interrupted')}")
            print("Saving current results...")
        
        # ä¿å­˜ç»“æœåˆ°Excel
        self.save_results_to_excel(results, output_path)
        self.save_tokens_to_csv(output_path)
        
        print(f"\n{get_message('parallel_processing_complete')}")
        print(get_message('processed_results_count', count=len(results)))
        
        return output_path
    
    def process_documents(self, folder_path, inclusion_criteria, output_path, exclusion_criteria=None, prompt_file_path=None):
        """å¤„ç†æ‰€æœ‰æ–‡æ¡£ - æ ¹æ®é…ç½®é€‰æ‹©å•çº¿ç¨‹æˆ–å¹¶è¡Œæ¨¡å¼"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            # Fallback if i18n not available
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print(f"\n{get_message('document_processing_mode', mode=self.screening_mode)}")
        
        if self.screening_mode == 'parallel' and self.parallel_screeners > 1:
            # ä½¿ç”¨å¹¶è¡Œå¤„ç†
            return self.process_documents_parallel(folder_path, inclusion_criteria, output_path, exclusion_criteria, prompt_file_path)
        else:
            # ä½¿ç”¨å•çº¿ç¨‹å¤„ç†
            return self.process_documents_sequential(folder_path, inclusion_criteria, output_path, exclusion_criteria, prompt_file_path)
    
    def process_documents_sequential(self, folder_path, inclusion_criteria, output_path, exclusion_criteria=None, prompt_file_path=None):
        """å•çº¿ç¨‹é¡ºåºå¤„ç†æ‰€æœ‰æ–‡æ¡£"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            # Fallback if i18n not available
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print(f"\n{get_message('sequential_processing_mode')}")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        file_patterns = [
            os.path.join(folder_path, "*.pdf"),
            os.path.join(folder_path, "*.docx"),
            os.path.join(folder_path, "*.doc"),
            os.path.join(folder_path, "*.txt")
        ]
        
        all_files = []
        for pattern in file_patterns:
            all_files.extend(glob.glob(pattern))
        
        if not all_files:
            print(get_message("system_error", error=f"No supported files found in {folder_path}"))
            return
        
        all_files.sort()
        total_files = len(all_files)
        print(get_message("detected_documents", count=total_files))
        
        # åˆ›å»ºç­›é€‰æç¤ºè¯
        if self.combined_prompt is None:
            print(get_message("prompt_generation"))
            self.combined_prompt = self.create_combined_prompt(inclusion_criteria, exclusion_criteria, prompt_file_path)
            print(get_message("prompt_loaded"))
        
        results = []
        start_time = time.time()
        
        try:
            for i, file_path in enumerate(all_files):
                file_name = os.path.basename(file_path)
                print(f"\n{get_message('current_document', filename=file_name)}")
                
                # è·å–æ–‡ä»¶å¤§å°
                try:
                    file_size_kb = os.path.getsize(file_path) / 1024
                    print(get_message("document_size", size=f"{file_size_kb:.1f}"))
                except:
                    pass
                
                # è¯»å–æ–‡æ¡£å†…å®¹
                print(get_message("pdf_extraction_started"))
                document_content = DocumentReader.read_file(file_path)
                if document_content is None:
                    print(get_message("document_failed", filename=file_name, error="unable to read file"))
                    continue
                
                # æ˜¾ç¤ºæå–çš„æ–‡æœ¬ä¿¡æ¯
                char_count = len(document_content)
                print(get_message("text_extracted", chars=f"{char_count:,}"))
                
                # æå–ç ”ç©¶ID
                # Get first valid LLM name (skip comment fields)
                valid_llm_names = [name for name in self.screening_llm_configs.keys() if not name.startswith('_')]
                llm_name = valid_llm_names[0] if valid_llm_names else list(self.screening_llm_configs.keys())[0]
                study_id = self.extract_study_id(document_content, llm_name)
                
                # å¤„ç†æ–‡æ¡£ï¼ˆæ— é¢„ç­›é€‰ï¼‰
                doc_start_time = time.time()
                print(get_message("llm_screening_started"))
                study_results = self.process_study_sequential(document_content, inclusion_criteria, study_id)
                doc_end_time = time.time()
                
                print(get_message("document_completed", filename=file_name))
                
                # æ˜¾ç¤ºè¿›åº¦ç»Ÿè®¡
                elapsed_time = doc_end_time - start_time
                avg_time_per_doc = elapsed_time / (i + 1)
                remaining_docs = total_files - (i + 1)
                estimated_remaining = avg_time_per_doc * remaining_docs
                
                print(get_message("documents_processed", processed=i+1, total=total_files))
                if i > 0:  # Only show speed after first document
                    speed = (i + 1) / (elapsed_time / 60)  # docs per minute
                    print(get_message("processing_speed", speed=f"{speed:.1f}"))
                    if remaining_docs > 0:
                        remaining_minutes = int(estimated_remaining / 60)
                        remaining_seconds = int(estimated_remaining % 60)
                        print(get_message("estimated_time", time=f"{remaining_minutes}m {remaining_seconds}s"))
                
                # ä¿å­˜ç»“æœ
                for llm_name, result in study_results.items():
                    result_data = {
                        'filename': file_name,
                        'filepath': file_path,
                        'study_id': study_id,
                        'llm_name': llm_name,
                        'picos': result['picos'],
                        'inclusion': result['inclusion']
                    }
                    results.append(result_data)
        
        except KeyboardInterrupt:
            print(f"\n{get_message('task_interrupted')}")
            print("Saving current results...")
        
        # ä¿å­˜ç»“æœåˆ°Excel
        print(f"\n{get_message('results_merging')}")
        self.save_results_to_excel(results, output_path)
        self.save_tokens_to_csv(output_path)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        print(f"\n{get_message('final_statistics')}")
        print(get_message('total_documents_processed', count=len(results) // len(self.screening_llm_configs)))
        print(get_message('total_processing_time', time=f"{int(total_time // 60)}m {int(total_time % 60)}s"))
        print(get_message('average_time_per_document', time=f"{total_time / max(1, len(results) // len(self.screening_llm_configs)):.1f}"))
        
        return output_path
    
    def process_fulltext_with_prompt(self, llm_name, document_content, prompt, study_id):
        """Process single full-text document with specified prompt"""
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            # Fallback if i18n not available
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        client = self.llm_clients[llm_name]
        config = self.screening_llm_configs[llm_name]
        
        # æˆªæ–­è¿‡é•¿çš„å†…å®¹
        estimated_tokens = len(document_content) // 4
        max_tokens = 28000  # é€‚ç”¨äºGPT-4å’Œä¸»æµå¤§æ¨¡å‹
        
        if estimated_tokens > max_tokens:
            print(f"  âš ï¸ Content too long (~{estimated_tokens:,} tokens), truncating to ~{max_tokens:,} tokens")
            # ä¿ç•™å¼€å¤´çš„75%å’Œç»“å°¾çš„25%å†…å®¹
            beginning_chars = int(max_tokens * 3)  # å‰75%
            ending_chars = int(max_tokens)  # å25%
            
            content_parts = []
            content_parts.append(document_content[:beginning_chars])
            content_parts.append("\n\n[...CONTENT TRUNCATED FOR LENGTH...]\n\n")
            content_parts.append(document_content[-ending_chars:])
            
            document_content = "".join(content_parts)
        
        print(get_message("sending_to_llm", llm_name=llm_name))
        
        max_retries = 10
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                response = client.chat.completions.create(
                    model=config['model'],
                    messages=[
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": document_content}
                    ],
                    temperature=0.0,
                    max_tokens=3000,
                    timeout=240
                )
                
                print(get_message("llm_response_received", llm_name=llm_name))
                result_text = response.choices[0].message.content
                
                # è®°å½•tokenæ¶ˆè€—
                try:
                    prompt_tokens = response.usage.prompt_tokens
                    completion_tokens = response.usage.completion_tokens
                    total_tokens = response.usage.total_tokens
                    
                    self.tokens_log.append({
                        'timestamp': datetime.now().isoformat(),
                        'operation': 'fulltext_screening',
                        'llm_name': llm_name,
                        'model': config['model'],
                        'prompt_tokens': prompt_tokens,
                        'completion_tokens': completion_tokens,
                        'total_tokens': total_tokens,
                        'study_id': study_id,
                        'document_length': len(document_content)
                    })
                except AttributeError:
                    estimated_prompt_tokens = len(prompt + document_content) // 4
                    estimated_completion_tokens = len(result_text) // 4
                    estimated_total_tokens = estimated_prompt_tokens + estimated_completion_tokens
                    
                    self.tokens_log.append({
                        'timestamp': datetime.now().isoformat(),
                        'operation': 'fulltext_screening',
                        'llm_name': llm_name,
                        'model': config['model'],
                        'prompt_tokens': estimated_prompt_tokens,
                        'completion_tokens': estimated_completion_tokens,
                        'total_tokens': estimated_total_tokens,
                        'study_id': study_id,
                        'document_length': len(document_content)
                    })
                
                # é›†æˆè§£æé€»è¾‘ï¼šä½¿ç”¨=== splitå’Œé€è¡Œè§£æï¼ˆå…¼å®¹æ ‡é¢˜æ‘˜è¦Promptï¼‰
                picos_section = ""
                decision_section = ""
                
                if "===PICOS EXTRACTION===" in result_text and "===INCLUSION ASSESSMENT===" in result_text:
                    sections = result_text.split("===INCLUSION ASSESSMENT===")
                    picos_section = sections[0].replace("===PICOS EXTRACTION===", "").strip()
                    decision_section = sections[1].strip()
                else:
                    picos_section = result_text
                    if "Decision:" in result_text:
                        decision_idx = result_text.find("Decision:")
                        decision_section = result_text[decision_idx:].strip()
                
                # Parse PICOS information
                picos = {}
                all_not_reported = True
                
                for line in picos_section.split('\n'):
                    line = line.strip()
                    if not line:
                        continue
                    
                    if line.startswith("Study design:"):
                        picos["study_design"] = line.split(":", 1)[1].strip()
                        if "not reported" not in picos["study_design"].lower():
                            all_not_reported = False
                    elif line.startswith("Participants:"):
                        picos["participants"] = line.split(":", 1)[1].strip()
                        if "not reported" not in picos["participants"].lower():
                            all_not_reported = False
                    elif line.startswith("Intervention:"):
                        picos["intervention"] = line.split(":", 1)[1].strip()
                        if "not reported" not in picos["intervention"].lower():
                            all_not_reported = False
                    elif line.startswith("Comparison:"):
                        picos["comparison"] = line.split(":", 1)[1].strip()
                        if "not reported" not in picos["comparison"].lower():
                            all_not_reported = False
                    elif line.startswith("Outcomes:"):
                        picos["outcomes"] = line.split(":", 1)[1].strip()
                        if "not reported" not in picos["outcomes"].lower():
                            all_not_reported = False
                
                # Parse inclusion decision and explanation
                inclusion_result = "â­•ï¸ UNCLEAR"
                explanation = ""
                short_reason = ""
                
                if "Explanation:" in decision_section:
                    parts = decision_section.split("Explanation:", 1)
                    explanation = parts[1].strip()
                    short_reason = explanation[:300]
                    if "." in short_reason:
                        short_reason = short_reason.split(".", 1)[0] + "."
                
                if "Decision:" in decision_section:
                    decision_line = decision_section.split("\n")[0]
                    decision_value = decision_line.split("Decision:", 1)[1].strip()
                else:
                    decision_value = "UNCLEAR"
                    
                if all_not_reported:
                    inclusion_result = "â­•ï¸ UNCLEAR (insufficient information)"
                else:
                    if "INCLUDE" in decision_value.upper():
                        inclusion_result = "âœ… INCLUDE"
                    elif "EXCLUDE" in decision_value.upper():
                        inclusion_result = f"âŒ EXCLUDE - {short_reason}"
                    else:
                        inclusion_result = f"â­•ï¸ UNCLEAR - {short_reason}"
                
                return picos, inclusion_result
                
            except Exception as e:
                if "timeout" in str(e).lower() or "timed out" in str(e).lower():
                    retry_count += 1
                    print(f"Timeout error with {llm_name}, attempt {retry_count}/{max_retries}: Response time exceeded 4 minutes. Retrying...")
                    if retry_count >= max_retries:
                        print(f"Failed after {max_retries} timeout attempts with {llm_name}")
                        return self._create_error_result(f"Response timeout after multiple attempts")
                else:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"Error with {llm_name}, attempt {retry_count}/{max_retries}: {str(e)}. Retrying...")
                        time.sleep(2 ** retry_count)
                    else:
                        print(f"Failed after {max_retries} attempts with {llm_name}: {str(e)}")
                        return self._create_error_result(f"Error in processing after multiple retries: {str(e)}")

    def _create_error_result(self, error_msg):
        """Create error result structure"""
        error_picos = {
            'study_design': f"Error: {error_msg}",
            'participants': f"Error: {error_msg}",
            'intervention': f"Error: {error_msg}",
            'comparison': f"Error: {error_msg}",
            'outcomes': f"Error: {error_msg}"
        }
        error_inclusion = f"â­• UNCLEAR (Error: {error_msg})"
        return error_picos, error_inclusion

    def save_results_to_excel(self, results, output_path):
        """Save results to Excel file with one row per study"""
        if not results:
            print("No results to save")
            return
        
        # æŒ‰æ–‡ä»¶ååˆ†ç»„ç»“æœ
        grouped_results = {}
        for result in results:
            filename = result['filename']
            if filename not in grouped_results:
                grouped_results[filename] = {
                    'filename': filename,
                    'study_id': result['study_id'],
                    'llm_results': {}
                }
            grouped_results[filename]['llm_results'][result['llm_name']] = result
        
        # åˆ›å»ºDataFrame
        data = []
        # Get valid LLM names (filter out comment fields)
        all_llm_names = list(self.screening_llm_configs.keys())
        llm_names = [name for name in all_llm_names if not name.startswith('_')]
        
        for filename, group_data in grouped_results.items():
            row = {
                'Filename': group_data['filename'],
                'Study ID': group_data['study_id'],
                'Last Update': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ä¸ºæ¯ä¸ªLLMæ·»åŠ åˆ—
            for llm_name in llm_names:
                if llm_name in group_data['llm_results']:
                    llm_result = group_data['llm_results'][llm_name]
                    row[f'Decision ({llm_name})'] = llm_result['inclusion']
                    row[f'Study Design ({llm_name})'] = llm_result['picos'].get('study_design', '')
                    row[f'Participants ({llm_name})'] = llm_result['picos'].get('participants', '')
                    row[f'Intervention ({llm_name})'] = llm_result['picos'].get('intervention', '')
                    row[f'Comparison ({llm_name})'] = llm_result['picos'].get('comparison', '')
                    row[f'Outcomes ({llm_name})'] = llm_result['picos'].get('outcomes', '')
                else:
                    # å¦‚æœæŸä¸ªLLMæ²¡æœ‰ç»“æœï¼Œå¡«å…¥ç©ºå€¼
                    row[f'Decision ({llm_name})'] = 'Not processed'
                    row[f'Study Design ({llm_name})'] = ''
                    row[f'Participants ({llm_name})'] = ''
                    row[f'Intervention ({llm_name})'] = ''
                    row[f'Comparison ({llm_name})'] = ''
                    row[f'Outcomes ({llm_name})'] = ''
            
            # æ·»åŠ å†²çªçŠ¶æ€
            row['Conflict Status'] = self._determine_conflict_status(group_data['llm_results'])
            
            data.append(row)
        
        df = pd.DataFrame(data)
        
        # ä¿å­˜åˆ°Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Full Text Results', index=False)
            
            # è·å–å·¥ä½œè¡¨è¿›è¡Œæ ¼å¼åŒ–
            worksheet = writer.sheets['Full Text Results']
            
            # è®¾ç½®åˆ—å®½
            worksheet.column_dimensions['A'].width = 25  # Filename
            worksheet.column_dimensions['B'].width = 15  # Study ID
            worksheet.column_dimensions['C'].width = 18  # Last Update
            
            # ä¸ºæ¯ä¸ªLLMçš„åˆ—è®¾ç½®å®½åº¦
            col_index = 4  # ä»ç¬¬4åˆ—å¼€å§‹
            for llm_name in llm_names:
                worksheet.column_dimensions[chr(64 + col_index)].width = 30      # Decision
                worksheet.column_dimensions[chr(64 + col_index + 1)].width = 20  # Study Design
                worksheet.column_dimensions[chr(64 + col_index + 2)].width = 25  # Participants
                worksheet.column_dimensions[chr(64 + col_index + 3)].width = 25  # Intervention
                worksheet.column_dimensions[chr(64 + col_index + 4)].width = 20  # Comparison
                worksheet.column_dimensions[chr(64 + col_index + 5)].width = 25  # Outcomes
                col_index += 6
            
            worksheet.column_dimensions[chr(64 + col_index)].width = 15  # Conflict Status
            
            # è®¾ç½®æ ‡é¢˜è¡Œæ ¼å¼
            header_fill = PatternFill(start_color="D9E1F2", end_color="D9E1F2", fill_type="solid")
            header_font = Font(bold=True)
            
            for col in range(1, len(df.columns) + 1):
                cell = worksheet.cell(row=1, column=col)
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(wrap_text=True, vertical="center")
            
            # è®¾ç½®æ•°æ®è¡Œæ ¼å¼å’Œé¢œè‰²
            for row in range(2, len(df) + 2):
                # è·å–å†²çªçŠ¶æ€
                conflict_status = worksheet.cell(row=row, column=len(df.columns)).value
                
                # è®¾ç½®è¡Œé¢œè‰²
                row_fill = None
                if conflict_status == "Conflict":
                    row_fill = PatternFill(start_color="FFCCCC", end_color="FFCCCC", fill_type="solid")  # çº¢è‰² - å†²çª
                elif conflict_status == "Uncertain":
                    row_fill = PatternFill(start_color="FFFF99", end_color="FFFF99", fill_type="solid")  # é»„è‰² - ä¸ç¡®å®š
                elif conflict_status == "Include":
                    row_fill = PatternFill(start_color="CCFFCC", end_color="CCFFCC", fill_type="solid")  # ç»¿è‰² - ä¸€è‡´çº³å…¥
                elif conflict_status == "Exclude":
                    row_fill = PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")  # ç°è‰² - ä¸€è‡´æ’é™¤
                
                # åº”ç”¨è¡Œæ ¼å¼
                for col in range(1, len(df.columns) + 1):
                    cell = worksheet.cell(row=row, column=col)
                    if row_fill:
                        cell.fill = row_fill
                    cell.alignment = Alignment(wrap_text=True, vertical="top")
            
            # å†»ç»“é¡¶éƒ¨è¡Œå’Œå‰3åˆ—
            worksheet.freeze_panes = "D2"
        
        # Import i18n functions
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            # Fallback if i18n not available
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print(get_message("results_saved", path=output_path))
        
        # æ˜¾ç¤ºç­›é€‰ç»Ÿè®¡
        self._display_screening_statistics(grouped_results)
        
        # æ˜¾ç¤ºTokenä½¿ç”¨ç»Ÿè®¡
        self._display_token_statistics()

    def _display_screening_statistics(self, grouped_results):
        """Display screening statistics"""
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        print(f"\nğŸ“Š {get_message('screening_criteria_loaded')}")
        
        # ç»Ÿè®¡å„ç§å†³ç­–ç»“æœ
        include_count = 0
        exclude_count = 0
        unclear_count = 0
        
        for filename, group_data in grouped_results.items():
            # è·å–ç¬¬ä¸€ä¸ªLLMçš„å†³ç­–ä½œä¸ºä¸»è¦å†³ç­–
            llm_results = group_data['llm_results']
            if llm_results:
                first_result = next(iter(llm_results.values()))
                decision = first_result['inclusion']
                
                if "âœ… INCLUDE" in decision or "INCLUDE" in decision.upper():
                    include_count += 1
                elif "âŒ EXCLUDE" in decision or "EXCLUDE" in decision.upper():
                    exclude_count += 1
                else:
                    unclear_count += 1
        
        print(get_message("documents_included", count=include_count))
        print(get_message("documents_excluded", count=exclude_count))
        print(get_message("documents_unclear", count=unclear_count))
        
        total_docs = include_count + exclude_count + unclear_count
        if total_docs > 0:
            print(f"  â€¢ Inclusion rate: {include_count/total_docs*100:.1f}%")
            print(f"  â€¢ Exclusion rate: {exclude_count/total_docs*100:.1f}%")
            if unclear_count > 0:
                print(f"  â€¢ Unclear rate: {unclear_count/total_docs*100:.1f}%")
    
    def _display_token_statistics(self):
        """Display token usage statistics"""
        try:
            from i18n.i18n_manager import get_message
        except ImportError:
            def get_message(key, **kwargs):
                return key.format(**kwargs) if kwargs else key
        
        if not self.tokens_log:
            return
        
        print(f"\nğŸ’° {get_message('token_usage')}")
        
        total_tokens = sum(entry['total_tokens'] for entry in self.tokens_log)
        total_prompt_tokens = sum(entry['prompt_tokens'] for entry in self.tokens_log)
        total_completion_tokens = sum(entry['completion_tokens'] for entry in self.tokens_log)
        
        print(get_message("total_tokens", total=f"{total_tokens:,}"))
        print(f"  â€¢ Prompt tokens: {total_prompt_tokens:,}")
        print(f"  â€¢ Completion tokens: {total_completion_tokens:,}")
        
        # è®¡ç®—å¹³å‡æ¯æ–‡æ¡£Tokenä½¿ç”¨
        unique_studies = len(set(entry['study_id'] for entry in self.tokens_log))
        if unique_studies > 0:
            avg_tokens_per_doc = total_tokens / unique_studies
            print(get_message("average_tokens_per_doc", avg=f"{avg_tokens_per_doc:,.0f}"))
        
        # ä¼°ç®—æˆæœ¬ï¼ˆåŸºäºå¸¸è§æ¨¡å‹å®šä»·ï¼‰
        estimated_cost = self._estimate_cost(total_prompt_tokens, total_completion_tokens)
        if estimated_cost > 0:
            print(get_message("tokens_used", tokens=f"{total_tokens:,}", cost=estimated_cost))
    
    def _estimate_cost(self, prompt_tokens, completion_tokens):
        """Estimate cost based on token usage"""
        # ç®€å•çš„æˆæœ¬ä¼°ç®—ï¼ŒåŸºäºå¸¸è§æ¨¡å‹å®šä»·
        # è¿™é‡Œä½¿ç”¨GPT-4çš„å®šä»·ä½œä¸ºå‚è€ƒï¼š$0.03/1K prompt tokens, $0.06/1K completion tokens
        prompt_cost = (prompt_tokens / 1000) * 0.03
        completion_cost = (completion_tokens / 1000) * 0.06
        return prompt_cost + completion_cost

    def _determine_conflict_status(self, llm_results):
        """Determine conflict status based on LLM decisions"""
        decisions = []
        for llm_name, result in llm_results.items():
            inclusion = result['inclusion']
            if "âœ… INCLUDE" in inclusion:
                decisions.append("INCLUDE")
            elif "âŒ EXCLUDE" in inclusion:
                decisions.append("EXCLUDE")
            else:
                decisions.append("UNCLEAR")
        
        # åˆ¤æ–­å†²çªçŠ¶æ€
        unique_decisions = set(decisions)
        
        if "UNCLEAR" in unique_decisions:
            return "Uncertain"  # è‡³å°‘ä¸€ä¸ªLLMä¸ç¡®å®š
        elif len(unique_decisions) > 1:
            return "Conflict"   # LLMä¹‹é—´æœ‰åˆ†æ­§
        elif "INCLUDE" in unique_decisions:
            return "Include"    # ä¸€è‡´çº³å…¥
        elif "EXCLUDE" in unique_decisions:
            return "Exclude"    # ä¸€è‡´æ’é™¤
        else:
            return "Unknown"


    def save_tokens_to_csv(self, output_path):
        """Save token consumption statistics to CSV file"""
        if not self.tokens_log:
            print("No token consumption records")
            return
        
        # ç”ŸæˆCSVæ–‡ä»¶è·¯å¾„
        if self.tokens_csv_path is None:
            base_name = os.path.splitext(output_path)[0]
            self.tokens_csv_path = f"{base_name}_tokens_usage.csv"
        
        # å†™å…¥CSVæ–‡ä»¶
        fieldnames = ['timestamp', 'operation', 'llm_name', 'model', 'prompt_tokens', 
                    'completion_tokens', 'total_tokens', 'study_id', 'document_length']
        
        with open(self.tokens_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.tokens_log)
        
        # è®¡ç®—æ€»è®¡
        total_prompt_tokens = sum(log['prompt_tokens'] for log in self.tokens_log)
        total_completion_tokens = sum(log['completion_tokens'] for log in self.tokens_log)
        total_tokens = sum(log['total_tokens'] for log in self.tokens_log)
        
        print(f"\n=== TOKEN CONSUMPTION STATISTICS ===")
        print(f"Detailed records saved to: {self.tokens_csv_path}")
        print(f"Total: {total_tokens:,} tokens (Input: {total_prompt_tokens:,}, Output: {total_completion_tokens:,})")
